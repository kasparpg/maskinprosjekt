{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install and import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asttokens==2.1.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: attrs==22.1.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (22.1.0)\n",
      "Requirement already satisfied: autopep8==2.0.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: catboost==1.1.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.1.1)\n",
      "Requirement already satisfied: certifi==2022.9.24 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (2022.9.24)\n",
      "Requirement already satisfied: click==8.1.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (8.1.3)\n",
      "Requirement already satisfied: click-plugins==1.1.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (1.1.1)\n",
      "Requirement already satisfied: cligj==0.7.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (0.7.2)\n",
      "Requirement already satisfied: colorama==0.4.6 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: contourpy==1.0.6 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (1.0.6)\n",
      "Requirement already satisfied: cycler==0.11.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (0.11.0)\n",
      "Requirement already satisfied: Cython==0.29.28 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (0.29.28)\n",
      "Requirement already satisfied: debugpy==1.6.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (1.6.3)\n",
      "Requirement already satisfied: decorator==5.1.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (5.1.1)\n",
      "Requirement already satisfied: entrypoints==0.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (0.4)\n",
      "Requirement already satisfied: executing==1.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (1.2.0)\n",
      "Requirement already satisfied: Fiona==1.8.22 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 18)) (1.8.22)\n",
      "Requirement already satisfied: fonttools==4.38.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 19)) (4.38.0)\n",
      "Requirement already satisfied: gensim==4.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 20)) (4.2.0)\n",
      "Requirement already satisfied: geographiclib==1.52 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 21)) (1.52)\n",
      "Requirement already satisfied: geopandas==0.12.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 22)) (0.12.1)\n",
      "Requirement already satisfied: geopy==2.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 23)) (2.2.0)\n",
      "Requirement already satisfied: graphviz==0.20.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 24)) (0.20.1)\n",
      "Requirement already satisfied: ipykernel==6.17.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 25)) (6.17.0)\n",
      "Requirement already satisfied: ipython==8.6.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 26)) (8.6.0)\n",
      "Requirement already satisfied: jedi==0.18.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 27)) (0.18.1)\n",
      "Requirement already satisfied: joblib==1.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 28)) (1.2.0)\n",
      "Requirement already satisfied: jupyter_client==7.4.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 29)) (7.4.4)\n",
      "Requirement already satisfied: jupyter_core==4.11.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 30)) (4.11.2)\n",
      "Requirement already satisfied: kiwisolver==1.4.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 31)) (1.4.4)\n",
      "Requirement already satisfied: matplotlib==3.6.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 32)) (3.6.0)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 33)) (0.1.6)\n",
      "Requirement already satisfied: munch==2.5.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 34)) (2.5.0)\n",
      "Requirement already satisfied: nest-asyncio==1.5.6 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 35)) (1.5.6)\n",
      "Requirement already satisfied: numpy==1.23.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 36)) (1.23.4)\n",
      "Requirement already satisfied: packaging==21.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 37)) (21.3)\n",
      "Requirement already satisfied: pandas==1.5.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 38)) (1.5.1)\n",
      "Requirement already satisfied: parso==0.8.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 39)) (0.8.3)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 40)) (0.7.5)\n",
      "Requirement already satisfied: Pillow==9.3.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 41)) (9.3.0)\n",
      "Requirement already satisfied: plotly==5.11.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 42)) (5.11.0)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.31 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 43)) (3.0.31)\n",
      "Requirement already satisfied: psutil==5.9.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 44)) (5.9.3)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 45)) (0.2.2)\n",
      "Requirement already satisfied: pyarrow==10.0.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 46)) (10.0.0)\n",
      "Requirement already satisfied: pycodestyle==2.9.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 47)) (2.9.1)\n",
      "Requirement already satisfied: pygeos==0.13 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 48)) (0.13)\n",
      "Requirement already satisfied: Pygments==2.13.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 49)) (2.13.0)\n",
      "Requirement already satisfied: pyparsing==3.0.9 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 50)) (3.0.9)\n",
      "Requirement already satisfied: pyproj==3.4.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 51)) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 52)) (2.8.2)\n",
      "Requirement already satisfied: pytz==2022.6 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 53)) (2022.6)\n",
      "Requirement already satisfied: pyzmq==24.0.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 54)) (24.0.1)\n",
      "Requirement already satisfied: scikit-learn==1.1.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 55)) (1.1.3)\n",
      "Requirement already satisfied: scipy==1.9.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 56)) (1.9.3)\n",
      "Requirement already satisfied: seaborn==0.12.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 57)) (0.12.1)\n",
      "Requirement already satisfied: Shapely==1.8.5.post1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 58)) (1.8.5.post1)\n",
      "Requirement already satisfied: six==1.16.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 59)) (1.16.0)\n",
      "Requirement already satisfied: sklearn==0.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 60)) (0.0)\n",
      "Requirement already satisfied: smart-open==6.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 61)) (6.2.0)\n",
      "Requirement already satisfied: stack-data==0.6.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 62)) (0.6.0)\n",
      "Requirement already satisfied: tenacity==8.1.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 63)) (8.1.0)\n",
      "Requirement already satisfied: threadpoolctl==3.1.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 64)) (3.1.0)\n",
      "Requirement already satisfied: tomli==2.0.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 65)) (2.0.1)\n",
      "Requirement already satisfied: tornado==6.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 66)) (6.2)\n",
      "Requirement already satisfied: tqdm==4.64.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 67)) (4.64.1)\n",
      "Requirement already satisfied: traitlets==5.5.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 68)) (5.5.0)\n",
      "Requirement already satisfied: wcwidth==0.2.5 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 69)) (0.2.5)\n",
      "Requirement already satisfied: xgboost==1.7.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 70)) (1.7.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from Fiona==1.8.22->-r requirements.txt (line 18)) (44.0.0)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in ./venv/lib/python3.8/site-packages (from ipython==8.6.0->-r requirements.txt (line 26)) (4.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.8/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython==8.6.0->-r requirements.txt (line 26)) (0.7.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import warnings\n",
    "import os.path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopy\n",
    "import os\n",
    "import shutil\n",
    "import geopandas as gpd\n",
    "import catboost as cb\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import geopandas as gpd\n",
    "\n",
    "from pyproj import Geod\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, LineString\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import make_scorer, mean_squared_log_error\n",
    "from sklearn.cluster import DBSCAN\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "from catboost.utils import get_gpu_device_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "SEED = 23\n",
    "\n",
    "spatial = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "age = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "income = pd.read_csv('data/grunnkrets_income_households.csv').set_index(['grunnkrets_id', 'year']).add_prefix('income_').reset_index()\n",
    "households = pd.read_csv('data/grunnkrets_households_num_persons.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "plaace = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "busstops = pd.read_csv('data/busstops_norway.csv')\n",
    "\n",
    "train = pd.read_csv('data/stores_train.csv')\n",
    "train_extra = pd.read_csv('data/stores_extra.csv')\n",
    "test = pd.read_csv('data/stores_test.csv') \n",
    "\n",
    "submission = pd.read_csv('data/sample_submission.csv')  # Please do not delete this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create geopandas version of some of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "busstops_geo = gpd.GeoDataFrame(busstops, geometry=busstops.geometry.apply(wkt.loads))\n",
    "\n",
    "train_geo = gpd.GeoDataFrame(train[['store_id', 'lon','lat']], geometry=gpd.points_from_xy(train.lon, train.lat)).drop(columns=['lon', 'lat'])\n",
    "train_geo = train_geo.set_crs('epsg:4326', allow_override=True).to_crs('epsg:3857')\n",
    "\n",
    "train_geo_extra = gpd.GeoDataFrame(train_extra[['store_id', 'lon','lat']], geometry=gpd.points_from_xy(train_extra.lon, train_extra.lat)).drop(columns=['lon', 'lat'])\n",
    "train_geo_extra = train_geo_extra.set_crs('epsg:4326', allow_override=True).to_crs('epsg:3857')\n",
    "\n",
    "test_geo = gpd.GeoDataFrame(test[['store_id', 'lon','lat']], geometry=gpd.points_from_xy(test.lon, test.lat)).drop(columns=['lon', 'lat'])\n",
    "test_geo = test_geo.set_crs('epsg:4326', allow_override=True).to_crs('epsg:3857')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    y_pred[y_pred < 0] = 0 + 1e-6\n",
    "    y_true[y_true < 0] = 0 + 1e-6\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "rmsle_scorer = make_scorer(lambda y, y_true: rmsle(y, y_true), greater_is_better=False)\n",
    "\n",
    "def to_categorical(df: pd.DataFrame):\n",
    "    for cat_col in df.select_dtypes(include=[object]).columns:\n",
    "        df[cat_col] = df[cat_col].astype('category')\n",
    "    return df\n",
    "\n",
    "\n",
    "def object_encoder(df: pd.DataFrame):\n",
    "    enc = OrdinalEncoder()\n",
    "    obj_cols = df.select_dtypes(include=[object]).columns\n",
    "    df[obj_cols] = enc.fit_transform(df[obj_cols])\n",
    "    return df\n",
    "\n",
    "\n",
    "def nan_to_string(df: pd.DataFrame):\n",
    "    nan = '#N/A'\n",
    "    cols = df[df.columns[df.isna().any()]].columns\n",
    "    df[cols] = df[cols].fillna(nan)\n",
    "    return df\n",
    "\n",
    "\n",
    "def meter_distance(lat1, lon1, lat2, lon2):\n",
    "    line_string = LineString([Point(lon1, lat1), Point(lon2, lat2)])\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "    return geod.geometry_length(line_string)\n",
    "\n",
    "\n",
    "def add_city_centre_dist(X: pd.DataFrame):\n",
    "    old_shape = X.shape\n",
    "\n",
    "    city_centres = X.groupby(['municipality_name'])[['lat', 'lon']].apply(lambda x: x.sum() / (x.count()))[['lat', 'lon']]\n",
    "    X = X.merge(city_centres, on=['municipality_name'], how='left', suffixes=(None, '_center'))\n",
    "    assert X.shape[0] == old_shape[0]\n",
    "\n",
    "    X.fillna(value={'lat_center': X.lat, 'lon_center': X.lon}, inplace=True)\n",
    "\n",
    "    X['dist_to_center'] = X.apply(lambda row: meter_distance(row.lat, row.lon, row.lat_center, row.lon_center), axis=1)\n",
    "    assert X.shape[0] == old_shape[0]\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def group_ages(age: pd.DataFrame, age_ranges: List[Tuple[int, int]]):\n",
    "    age_new = age[['grunnkrets_id', 'year']].drop_duplicates(subset=['grunnkrets_id'], keep='last')\n",
    "\n",
    "    for rng in age_ranges:\n",
    "        cols = [f'age_{age}' for age in range(rng[0], rng[1] + 1)]\n",
    "        rng_sum = age[cols].sum(axis=1).astype(int)\n",
    "        age_new[f'age_{rng[0]}_{rng[-1]}'] = rng_sum\n",
    "\n",
    "    age = age.drop_duplicates(subset='grunnkrets_id').drop(columns=['year', *(f'age_{age}' for age in range(0, 91))], axis=1)\n",
    "    age = age.merge(age_new.drop(columns=['year']), on='grunnkrets_id')\n",
    "\n",
    "    return age\n",
    "\n",
    "\n",
    "def only_latest_data(df: pd.DataFrame):\n",
    "    df = df.sort_values(by='year', ascending=False)\n",
    "    df = df.drop_duplicates(subset='grunnkrets_id', keep='first')\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_out_nan_heavy_rows(df: pd.DataFrame, age, age_ranges, spatial_2016, income_2016, households_2016):\n",
    "    \"\"\"Cleans out rows that have no match in the age, spatial, income or household datasets.\"\"\"\n",
    "\n",
    "    # df2 = df.merge(group_ages(age, age_ranges), on='grunnkrets_id', how='left')\n",
    "    df2 = df.merge(spatial_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(income_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(households_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "\n",
    "    df_cleaned = df2[\n",
    "        ~(df2.couple_children_0_to_5_years.isna() | df2.grunnkrets_name.isna() | df2.income_all_households.isna()) # df2.age_0_19.isna() | \n",
    "    ]\n",
    "\n",
    "    print(f'Cleaned out {len(df) - len(df_cleaned)} out of {len(df)} rows.')\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def create_busstops_files():\n",
    "    \"\"\"\n",
    "    Creates a .parquet file that stores info aboute the number of busstops within a 1 kilometer\n",
    "    radius of a given store, as well as the number of stops within the different importance categories.  \n",
    "    \"\"\"\n",
    "\n",
    "    def bus_fields(row: pd.Series):\n",
    "        output_dict = {}\n",
    "        gpd.options.use_pygeos = True\n",
    "        \n",
    "        stops_with_dist = busstops[busstops_geo.distance(row.geometry) < 1000]\n",
    "        output_dict['bus_stops_count'] = len(stops_with_dist)\n",
    "        \n",
    "        output_dict.update(stops_with_dist.importance_level.value_counts().reindex(\n",
    "            busstops_geo.importance_level.unique(), fill_value=0\n",
    "        ).to_dict())\n",
    "        \n",
    "        return output_dict\n",
    "\n",
    "    train_with_extras_bus = pd.concat([train_geo, train_geo_extra], ignore_index=True)\n",
    "\n",
    "    train_with_extras_bus = train_with_extras_bus.join(train_with_extras_bus.progress_apply(lambda row: bus_fields(row), axis=1, result_type='expand'))\n",
    "    train_with_extras_bus.to_parquet('derived_data/stores_bus_stops_lt_1km_train.parquet')\n",
    "\n",
    "    test_bus = test_bus.join(test_bus.progress_apply(lambda row: bus_fields(row), axis=1, result_type='expand'))\n",
    "    test_bus.to_parquet('derived_data/stores_bus_stops_lt_1km_test.parquet')\n",
    "\n",
    "\n",
    "def create_stores_in_vicinity_files():\n",
    "    \"\"\"\n",
    "    Creates a .parquet file that stores info aboute the number of other stores within a 1 kilometer\n",
    "    radius of a given store.\n",
    "    \"\"\"\n",
    "\n",
    "    train_with_extras = pd.concat([train_geo, train_geo_extra], ignore_index=True)[['store_id', 'geometry']]\n",
    "\n",
    "    def store_count_in_vicinity(row: pd.Series):\n",
    "        stores_in_vicinity = train_with_extras[train_with_extras.distance(row.geometry) < 1000]\n",
    "        return {'stores_count_lt_1km': len(stores_in_vicinity)}\n",
    "\n",
    "    train_with_extras = train_with_extras.join(train_with_extras.progress_apply(lambda row: store_count_in_vicinity(row), axis=1, result_type='expand'))\n",
    "    train_with_extras.to_parquet('derived_data/stores_count_lt_1km_train.parquet')\n",
    "\n",
    "    test = test.join(test.progress_apply(lambda row: store_count_in_vicinity(row), axis=1, result_type='expand'))\n",
    "    test.to_parquet('derived_data/stores_count_lt_1km_test.parquet')\n",
    "\n",
    "\n",
    "def add_spatial_clusters(df: pd.DataFrame):\n",
    "    clusters = DBSCAN(eps=0.145, min_samples=100)\n",
    "    # clusters = DBSCAN(eps=0.12, min_samples=30)\n",
    "    cl = clusters.fit_predict(df[['lat', 'lon']].to_numpy())\n",
    "    cl_counts = dict(zip(*np.unique(cl, return_counts=True)))\n",
    "\n",
    "    print(len(set(cl)), 'clusters created')\n",
    "    print('Cluster counts:', cl_counts)\n",
    "\n",
    "    df['cluster_id'] = cl\n",
    "    df['cluster_member_count'] = df.apply(lambda row: cl_counts[row.cluster_id], axis=1)\n",
    "\n",
    "    X_no_outliers = df[df.cluster_id != -1]\n",
    "    cluster_centroids = X_no_outliers.groupby('cluster_id')[['lat', 'lon']].mean()\n",
    "\n",
    "    def closest_centroid(lat, lon):\n",
    "        dist_series = cluster_centroids.apply(lambda row: meter_distance(lat, lon, row.lat, row.lon), axis=1)\n",
    "        return dist_series.min()\n",
    "\n",
    "    print('Calculating distance to closest cluster for each data point...')\n",
    "    df['closest_cluster_centroid_dist'] = df.progress_apply(lambda row: closest_centroid(row.lat, row.lon), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data only contains data from 2016, so for the other datasets with an age column\n",
    "we only use the values from 2016, where possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_ranges = [\n",
    "    (0, 19),\n",
    "    (20, 39),\n",
    "    (40, 59),\n",
    "    (60, 79),\n",
    "    (80, 90),\n",
    "]\n",
    "\n",
    "spatial_latest = only_latest_data(spatial)\n",
    "income_latest = only_latest_data(income)\n",
    "households_latest = only_latest_data(households)\n",
    "\n",
    "train_spatial = train.merge(spatial_latest.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "muni_avg_revenue = train_spatial.groupby(by='municipality_name', as_index=False)['revenue'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Futhermore, we noticed that a number of rows in the train and test datasets didn't have  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 clusters created\n",
      "Cluster counts: {-1: 5882, 0: 7293, 1: 155, 2: 219, 3: 823, 4: 1156, 5: 104, 6: 1413, 7: 181, 8: 143, 9: 139, 10: 414, 11: 310, 12: 385, 13: 520, 14: 515, 15: 211, 16: 180, 17: 414, 18: 315, 19: 113, 20: 113, 21: 105, 22: 126, 23: 103, 24: 104}\n",
      "Calculating distance to closest cluster for each data point...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21436/21436 [01:01<00:00, 350.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned out 127 out of 12859 rows.\n",
      "Length of data frame: 10185\n",
      "Length after removing extreme values and zero revenue retail stores: 9947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def clean(df: pd.DataFrame, min_val=0, max_val=100):\n",
    "    print('Length of data frame:', len(df))\n",
    "    df = df[(df.revenue > min_val) & (df.revenue < max_val)]\n",
    "    print('Length after removing extreme values and zero revenue retail stores:',  len(df))\n",
    "    return df.drop(columns=['revenue']), df.revenue\n",
    "\n",
    "\n",
    "def clean_out_nan_heavy_rows(df: pd.DataFrame):\n",
    "    \"\"\"Cleans out rows that have no match in the age, spatial, income or household datasets.\"\"\"\n",
    "\n",
    "    # df2 = df.merge(group_ages(age, age_ranges), on='grunnkrets_id', how='left')\n",
    "    df2 = df.merge(spatial_latest.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(income_latest.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(households_latest.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "\n",
    "    df_cleaned = df[\n",
    "        ~(df2.couple_children_0_to_5_years.isna() | df2.grunnkrets_name.isna() | df2.income_all_households.isna())  # | df2.age_0_19.isna() \n",
    "    ]\n",
    "\n",
    "    print(f'Cleaned out {len(df) - len(df_cleaned)} out of {len(df)} rows.')\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "train_test_clustered = add_spatial_clusters(pd.concat([train.drop(columns=['revenue']), test], axis=0).reset_index())\n",
    "train_test_clustered = train_test_clustered[['store_id', 'cluster_id', 'cluster_member_count', 'closest_cluster_centroid_dist']]\n",
    "\n",
    "train = train.merge(train_test_clustered, on='store_id', how='left')\n",
    "test = test.merge(train_test_clustered, on='store_id', how='left')\n",
    "\n",
    "train = clean_out_nan_heavy_rows(train)\n",
    "label_name = 'revenue'\n",
    "X = train.drop(columns=[label_name])\n",
    "y = train[label_name]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=.8, random_state=SEED)\n",
    "X_train, y_train = clean(pd.merge(X_train, y_train, left_index=True, right_index=True))\n",
    "\n",
    "y_train = np.log1p(y_train)\n",
    "y_val = np.log1p(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p derived_data\n",
    "\n",
    "if len(os.listdir('derived_data')) == 0:\n",
    "    create_busstops_files()\n",
    "    create_stores_in_vicinity_files()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df: pd.DataFrame, data_origin: str, predictor: str = ''):\n",
    "    # Define datasets to be merged\n",
    "    age_groups_merge = group_ages(age, age_ranges)\n",
    "    spatial_merge = spatial_latest.drop(columns=['year'])\n",
    "    income_merge = income_latest.drop(columns=['year'])\n",
    "    households_merge = households_latest.drop(columns=['year'])\n",
    "    plaace_merge = plaace.drop_duplicates(subset='plaace_hierarchy_id')\n",
    "    bus_data_train_merge = gpd.read_parquet(f'derived_data/stores_bus_stops_lt_1km_{data_origin}.parquet').drop(columns=['geometry'])\n",
    "    stores_vicinity_merge = gpd.read_parquet(f'derived_data/stores_count_lt_1km_{data_origin}.parquet').drop(columns=['geometry'])\n",
    "\n",
    "    # Merge datasets\n",
    "    df = df.merge(age_groups_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(spatial_merge, on='grunnkrets_id', how='left')\n",
    "    # df = df.merge(muni_avg_revenue, on='municipality_name', how='left', suffixes=(None, '_muni_avg'))\n",
    "    df = df.merge(income_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(households_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(plaace_merge, how='left')\n",
    "    df = df.merge(bus_data_train_merge, on='store_id', how='left')\n",
    "    df = df.merge(stores_vicinity_merge, on='store_id', how='left')\n",
    "    df = add_city_centre_dist(df).drop(columns=['lon_center', 'lat_center'])\n",
    "\n",
    "    # Transformations and some post-merge cleaning\n",
    "    df.stores_count_lt_1km = np.log(df.stores_count_lt_1km)\n",
    "    df.closest_cluster_centroid_dist = np.log(df.closest_cluster_centroid_dist)\n",
    "    df[age_groups_merge.columns] = df[age_groups_merge.columns].fillna(0)\n",
    "    \n",
    "\n",
    "    # Handle categories for different predictors\n",
    "    if predictor == 'xgb':\n",
    "        # df = to_categorical(df)\n",
    "        df = object_encoder(df)\n",
    "    elif predictor == 'cb':\n",
    "        df = nan_to_string(df)\n",
    "    elif predictor == 'lgb':\n",
    "        df = to_categorical(df)\n",
    "    else: \n",
    "        raise ValueError('Invalid predictor')\n",
    "\n",
    "    features = [\n",
    "        'store_name', \n",
    "        'mall_name', \n",
    "        'chain_name',\n",
    "        'address', \n",
    "        'lat', 'lon',\n",
    "        \n",
    "        *age_groups_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        *income_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        *households_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        'lv1_desc', 'lv2_desc', 'sales_channel_name',  # 'lv3_desc', 'lv4_desc',\n",
    "        *bus_data_train_merge.drop(columns=['store_id']).columns,\n",
    "        *stores_vicinity_merge.drop(columns=['store_id']).columns,\n",
    "        'dist_to_center',\n",
    "        'cluster_id', 'cluster_member_count', 'closest_cluster_centroid_dist'\n",
    "    ]\n",
    "\n",
    "    return df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "derived_data/stores_bus_stops_lt_1km_train",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Features adapted to Catboost\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train_cb \u001b[39m=\u001b[39m generate_features(X_train, data_origin\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, predictor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m X_val_cb \u001b[39m=\u001b[39m generate_features(X_val, data_origin\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, predictor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m X_test_cb \u001b[39m=\u001b[39m generate_features(test, data_origin\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, predictor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcb\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [10], line 8\u001b[0m, in \u001b[0;36mgenerate_features\u001b[0;34m(df, data_origin, predictor)\u001b[0m\n\u001b[1;32m      6\u001b[0m households_merge \u001b[39m=\u001b[39m households_latest\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m plaace_merge \u001b[39m=\u001b[39m plaace\u001b[39m.\u001b[39mdrop_duplicates(subset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mplaace_hierarchy_id\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m bus_data_train_merge \u001b[39m=\u001b[39m gpd\u001b[39m.\u001b[39;49mread_parquet(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mderived_data/stores_bus_stops_lt_1km_\u001b[39;49m\u001b[39m{\u001b[39;49;00mdata_origin\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mgeometry\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m stores_vicinity_merge \u001b[39m=\u001b[39m gpd\u001b[39m.\u001b[39mread_parquet(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mderived_data/stores_count_lt_1km_\u001b[39m\u001b[39m{\u001b[39;00mdata_origin\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mgeometry\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[39m# Merge datasets\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/geopandas/io/arrow.py:560\u001b[0m, in \u001b[0;36m_read_parquet\u001b[0;34m(path, columns, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m path \u001b[39m=\u001b[39m _expand_user(path)\n\u001b[1;32m    559\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_pandas_metadata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m table \u001b[39m=\u001b[39m parquet\u001b[39m.\u001b[39;49mread_table(path, columns\u001b[39m=\u001b[39;49mcolumns, filesystem\u001b[39m=\u001b[39;49mfilesystem, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    562\u001b[0m \u001b[39m# read metadata separately to get the raw Parquet FileMetaData metadata\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[39m# (pyarrow doesn't properly exposes those in schema.metadata for files\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[39m# created by GDAL - https://issues.apache.org/jira/browse/ARROW-16688)\u001b[39;00m\n\u001b[1;32m    565\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/pyarrow/parquet/core.py:2824\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size, partitioning, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[1;32m   2817\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2818\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m keyword is no longer supported with the new \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2819\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdatasets-based implementation. Specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2820\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39muse_legacy_dataset=True\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to temporarily recover the old \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2821\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbehaviour.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2822\u001b[0m     )\n\u001b[1;32m   2823\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2824\u001b[0m     dataset \u001b[39m=\u001b[39m _ParquetDatasetV2(\n\u001b[1;32m   2825\u001b[0m         source,\n\u001b[1;32m   2826\u001b[0m         schema\u001b[39m=\u001b[39;49mschema,\n\u001b[1;32m   2827\u001b[0m         filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m   2828\u001b[0m         partitioning\u001b[39m=\u001b[39;49mpartitioning,\n\u001b[1;32m   2829\u001b[0m         memory_map\u001b[39m=\u001b[39;49mmemory_map,\n\u001b[1;32m   2830\u001b[0m         read_dictionary\u001b[39m=\u001b[39;49mread_dictionary,\n\u001b[1;32m   2831\u001b[0m         buffer_size\u001b[39m=\u001b[39;49mbuffer_size,\n\u001b[1;32m   2832\u001b[0m         filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[1;32m   2833\u001b[0m         ignore_prefixes\u001b[39m=\u001b[39;49mignore_prefixes,\n\u001b[1;32m   2834\u001b[0m         pre_buffer\u001b[39m=\u001b[39;49mpre_buffer,\n\u001b[1;32m   2835\u001b[0m         coerce_int96_timestamp_unit\u001b[39m=\u001b[39;49mcoerce_int96_timestamp_unit,\n\u001b[1;32m   2836\u001b[0m         thrift_string_size_limit\u001b[39m=\u001b[39;49mthrift_string_size_limit,\n\u001b[1;32m   2837\u001b[0m         thrift_container_size_limit\u001b[39m=\u001b[39;49mthrift_container_size_limit,\n\u001b[1;32m   2838\u001b[0m     )\n\u001b[1;32m   2839\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m   2840\u001b[0m     \u001b[39m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[1;32m   2841\u001b[0m     \u001b[39m# module is not available\u001b[39;00m\n\u001b[1;32m   2842\u001b[0m     \u001b[39mif\u001b[39;00m filters \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/pyarrow/parquet/core.py:2423\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.__init__\u001b[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2419\u001b[0m \u001b[39mif\u001b[39;00m partitioning \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhive\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   2420\u001b[0m     partitioning \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mHivePartitioning\u001b[39m.\u001b[39mdiscover(\n\u001b[1;32m   2421\u001b[0m         infer_dictionary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m-> 2423\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mdataset(path_or_paths, filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m   2424\u001b[0m                            schema\u001b[39m=\u001b[39;49mschema, \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49mparquet_format,\n\u001b[1;32m   2425\u001b[0m                            partitioning\u001b[39m=\u001b[39;49mpartitioning,\n\u001b[1;32m   2426\u001b[0m                            ignore_prefixes\u001b[39m=\u001b[39;49mignore_prefixes)\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/pyarrow/dataset.py:752\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    741\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    742\u001b[0m     schema\u001b[39m=\u001b[39mschema,\n\u001b[1;32m    743\u001b[0m     filesystem\u001b[39m=\u001b[39mfilesystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    748\u001b[0m     selector_ignore_prefixes\u001b[39m=\u001b[39mignore_prefixes\n\u001b[1;32m    749\u001b[0m )\n\u001b[1;32m    751\u001b[0m \u001b[39mif\u001b[39;00m _is_path_like(source):\n\u001b[0;32m--> 752\u001b[0m     \u001b[39mreturn\u001b[39;00m _filesystem_dataset(source, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    753\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(source, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)):\n\u001b[1;32m    754\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(_is_path_like(elem) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m source):\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/pyarrow/dataset.py:444\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    442\u001b[0m     fs, paths_or_selector \u001b[39m=\u001b[39m _ensure_multiple_sources(source, filesystem)\n\u001b[1;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     fs, paths_or_selector \u001b[39m=\u001b[39m _ensure_single_source(source, filesystem)\n\u001b[1;32m    446\u001b[0m options \u001b[39m=\u001b[39m FileSystemFactoryOptions(\n\u001b[1;32m    447\u001b[0m     partitioning\u001b[39m=\u001b[39mpartitioning,\n\u001b[1;32m    448\u001b[0m     partition_base_dir\u001b[39m=\u001b[39mpartition_base_dir,\n\u001b[1;32m    449\u001b[0m     exclude_invalid_files\u001b[39m=\u001b[39mexclude_invalid_files,\n\u001b[1;32m    450\u001b[0m     selector_ignore_prefixes\u001b[39m=\u001b[39mselector_ignore_prefixes\n\u001b[1;32m    451\u001b[0m )\n\u001b[1;32m    452\u001b[0m factory \u001b[39m=\u001b[39m FileSystemDatasetFactory(fs, paths_or_selector, \u001b[39mformat\u001b[39m, options)\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/pyarrow/dataset.py:420\u001b[0m, in \u001b[0;36m_ensure_single_source\u001b[0;34m(path, filesystem)\u001b[0m\n\u001b[1;32m    418\u001b[0m     paths_or_selector \u001b[39m=\u001b[39m [path]\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(path)\n\u001b[1;32m    422\u001b[0m \u001b[39mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: derived_data/stores_bus_stops_lt_1km_train"
     ]
    }
   ],
   "source": [
    "# Features adapted to Catboost\n",
    "X_train_cb = generate_features(X_train, data_origin='train', predictor='cb')\n",
    "X_val_cb = generate_features(X_val, data_origin='train', predictor='cb')\n",
    "X_test_cb = generate_features(test, data_origin='test', predictor='cb')\n",
    "\n",
    "# Features adapted to LightGBM\n",
    "X_train_lgb = generate_features(X_train, data_origin='train', predictor='lgb')\n",
    "X_val_lgb = generate_features(X_val, data_origin='train', predictor='lgb')\n",
    "X_test_lgb = generate_features(test, data_origin='test', predictor='lgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing pools and parameter grid for Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cb_pools():\n",
    "    text_features = ['store_name', 'address', 'sales_channel_name'] \n",
    "    cat_features = ['mall_name', 'chain_name', 'lv1_desc', 'lv2_desc', 'cluster_id']\n",
    "\n",
    "    train_pool = cb.Pool(\n",
    "        X_train_cb,\n",
    "        y_train,\n",
    "        cat_features=cat_features,\n",
    "        text_features=text_features,\n",
    "        feature_names=list(X_train_cb)\n",
    "    )\n",
    "\n",
    "    valid_pool = cb.Pool(\n",
    "        X_val_cb,\n",
    "        y_val,\n",
    "        cat_features=cat_features,\n",
    "        text_features=text_features,\n",
    "        feature_names=list(X_train_cb)\n",
    "    )\n",
    "\n",
    "    return train_pool, valid_pool\n",
    "\n",
    "\n",
    "def get_cb_params(trial: optuna.Trial = None):\n",
    "    gpu_count = get_gpu_device_count()\n",
    "    non_tunable_cb_params = {\n",
    "        'objective': 'RMSE',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'task_type': 'GPU' if gpu_count else 'CPU', \n",
    "        'devices': f'0:{gpu_count}',\n",
    "        'random_seed': SEED\n",
    "    }\n",
    "\n",
    "    if trial is None:\n",
    "        return 'cb', non_tunable_cb_params\n",
    "    \n",
    "    tunable_params = {\n",
    "        'depth': trial.suggest_int('depth', 4, 9),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['Ordered', 'Plain']),\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 2, 6),\n",
    "        # 'iterations': trial.suggest_int('iterations', 1000, 2000),\n",
    "        # 'learning_rate': trial.suggest_categorical('learning_rate', 0.1, 0.5)\n",
    "    }\n",
    "\n",
    "    return 'cb', non_tunable_cb_params, tunable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing DMatrices and parameter grid for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lgb_dmatrices():\n",
    "    dtrain = lgb.Dataset(X_train_lgb, y_train, params={'verbose': -1}, free_raw_data=False)\n",
    "    dvalid = lgb.Dataset(X_val_lgb, y_val, params={'verbose': -1}, free_raw_data=False)\n",
    "    return dtrain, dvalid\n",
    "\n",
    "\n",
    "def get_lgb_params(trial: optuna.Trial = None):\n",
    "    non_tunable_lgb_params = {\n",
    "        'objective': 'rmse',\n",
    "        'verbose': -1,\n",
    "        'seed': 1\n",
    "    }\n",
    "\n",
    "    if trial is None:\n",
    "        return 'lgb', non_tunable_lgb_params\n",
    "\n",
    "    tunable_params = {\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'goss', 'dart']),\n",
    "    }\n",
    "\n",
    "    if tunable_params['boosting_type'] != 'goss':\n",
    "        tunable_params[\"bagging_fraction\"]: trial.suggest_float(\"bagging_fraction\", 0.4, 1.0)\n",
    "        tunable_params[\"bagging_freq\"]: trial.suggest_int(\"bagging_freq\", 1, 7)\n",
    "\n",
    "    return 'lgb', non_tunable_lgb_params, tunable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(trial: optuna.Trial, param_grid_fn: Callable) -> float:\n",
    "    model_name, non_tunable_params, tunable_params = param_grid_fn(trial)\n",
    "\n",
    "    if model_name == 'cb':\n",
    "        if tunable_params['bootstrap_type'] == 'Bayesian': \n",
    "            tunable_params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n",
    "        elif tunable_params['bootstrap_type'] == 'Bernoulli':\n",
    "            tunable_params['subsample'] = trial.suggest_float('subsample', 0.1, 1, log=True)\n",
    "\n",
    "        cbr = cb.CatBoostRegressor(**non_tunable_params, **tunable_params) \n",
    "        train_pool, valid_pool = get_cb_pools()\n",
    "        cbr.fit(\n",
    "            train_pool,\n",
    "            eval_set=[(X_val_cb, y_val)],\n",
    "            verbose=0,\n",
    "            early_stopping_rounds=100,\n",
    "        )\n",
    "        y_pred = cbr.predict(X_val_cb)\n",
    "    \n",
    "    elif model_name == 'lgb':\n",
    "        dtrain_lgb, dvalid_lgb = get_lgb_dmatrices()\n",
    "        lgbr = lgb.train(\n",
    "            params={**non_tunable_params, **tunable_params},\n",
    "            train_set=dtrain_lgb,\n",
    "            valid_sets=dvalid_lgb,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "        y_pred = lgbr.predict(X_val_lgb)\n",
    "\n",
    "    score = rmsle(np.expm1(y_val), np.expm1(y_pred))\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_hyper_parameters(param_grid_fn: Callable, n_trials=100):\n",
    "    study = optuna.create_study(\n",
    "        study_name='hyperparam-tuning',\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), \n",
    "        direction='minimize'\n",
    "    )\n",
    "    objective_fn = lambda trial: objective(trial, param_grid_fn)\n",
    "    study.optimize(objective_fn, n_trials=n_trials, timeout=900) \n",
    "\n",
    "    print('Number of finished trials: {}'.format(len(study.trials)))\n",
    "    \n",
    "    trial = study.best_trial\n",
    "    print(f'Best trial ({trial.number}):')\n",
    "    print('Value:', trial.value)\n",
    "    print('Params:')\n",
    "    print(trial.params)\n",
    "\n",
    "    return param_grid_fn()[1], trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_tunable_cb_params, tuned_params = get_hyper_parameters(get_cb_params, n_trials=30)\n",
    "train_pool, valid_pool = get_cb_pools()\n",
    "cbm = cb.CatBoostRegressor(**non_tunable_cb_params, **tuned_params, iterations=1000) \n",
    "cbm.fit(train_pool, eval_set=valid_pool, verbose=50, plot=True, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_tunable_lgb_params, tunable_lgb_params = get_hyper_parameters(get_lgb_params, n_trials=400)\n",
    "dtrain_lgb, dvalid_lgb = get_lgb_dmatrices()\n",
    "lgbm = lgb.train(\n",
    "    params={**non_tunable_lgb_params, **tunable_lgb_params},\n",
    "    train_set=dtrain_lgb,\n",
    "    valid_sets=dvalid_lgb,\n",
    "    verbose_eval=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost validation prediction\n",
    "y_pred_val_cb = np.expm1(cbm.predict(X_val_cb))\n",
    "print('Catboost validation score:', rmsle(np.expm1(y_val), y_pred_val_cb))\n",
    "\n",
    "# LightGBM validation prediction\n",
    "y_val_pred_lgb = np.expm1(lgbm.predict(X_val_lgb))\n",
    "print('LightGBM validation score:', rmsle(np.expm1(y_val), y_val_pred_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_cb = np.expm1(cbm.predict(X_test_cb))\n",
    "y_pred_test_lgb = np.expm1(lgbm.predict(X_test_lgb))\n",
    "\n",
    "test_stack = np.array([y_pred_test_cb, y_pred_test_lgb])\n",
    "stack_test_avg = np.mean(test_stack, axis=0)\n",
    "\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['predicted'] = stack_test_avg\n",
    "submission.to_csv('submissions/submission_lordag_kveld.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1beb4c19f9c9850d9088b15a4a1b3063555a573f9fb8d1ae667cbe7a8ada917e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
