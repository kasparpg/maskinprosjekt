{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Install required packages\n",
    "\n",
    "We ran into some issues with some of the geo related packages, such as geopandas, on certain machines. If the code does not run on your machine, you can comment out the lines commented with \"# GEO PROBLEM HERE\", and it will hopefully allow the code to run. This may of course impact the final score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgeopandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mgpd\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcatboost\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mcb\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39moptuna\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlightgbm\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mlgb\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyproj\u001b[39;00m \u001b[39mimport\u001b[39;00m Geod\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/__init__.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m \u001b[39mimport\u001b[39;00m integration\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m \u001b[39mimport\u001b[39;00m multi_objective\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m \u001b[39mimport\u001b[39;00m pruners\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m \u001b[39mimport\u001b[39;00m samplers\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/multi_objective/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_imports\u001b[39;00m \u001b[39mimport\u001b[39;00m _LazyImport\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmulti_objective\u001b[39;00m \u001b[39mimport\u001b[39;00m samplers  \u001b[39m# NOQA\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmulti_objective\u001b[39;00m \u001b[39mimport\u001b[39;00m study  \u001b[39m# NOQA\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmulti_objective\u001b[39;00m \u001b[39mimport\u001b[39;00m trial  \u001b[39m# NOQA\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/multi_objective/samplers/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmulti_objective\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msamplers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_adapter\u001b[39;00m \u001b[39mimport\u001b[39;00m _MultiObjectiveSamplerAdapter  \u001b[39m# NOQA\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmulti_objective\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msamplers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_base\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseMultiObjectiveSampler  \u001b[39m# NOQA\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmulti_objective\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msamplers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_motpe\u001b[39;00m \u001b[39mimport\u001b[39;00m MOTPEMultiObjectiveSampler  \u001b[39m# NOQA\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/multi_objective/samplers/_adapter.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m \u001b[39mimport\u001b[39;00m multi_objective\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseDistribution\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msamplers\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseSampler\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstudy\u001b[39;00m \u001b[39mimport\u001b[39;00m Study\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrial\u001b[39;00m \u001b[39mimport\u001b[39;00m FrozenTrial\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/samplers/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msamplers\u001b[39;00m \u001b[39mimport\u001b[39;00m nsgaii\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msamplers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_base\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseSampler\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msamplers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_cmaes\u001b[39;00m \u001b[39mimport\u001b[39;00m CmaEsSampler\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/samplers/nsgaii/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msamplers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnsgaii\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_crossovers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_base\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseCrossover\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msamplers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnsgaii\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_crossovers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_blxalpha\u001b[39;00m \u001b[39mimport\u001b[39;00m BLXAlphaCrossover\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msamplers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnsgaii\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_crossovers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_sbx\u001b[39;00m \u001b[39mimport\u001b[39;00m SBXCrossover\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/samplers/nsgaii/_crossovers/_base.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mabc\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstudy\u001b[39;00m \u001b[39mimport\u001b[39;00m Study\n\u001b[1;32m      8\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBaseCrossover\u001b[39;00m(\u001b[39mobject\u001b[39m, metaclass\u001b[39m=\u001b[39mabc\u001b[39m.\u001b[39mABCMeta):\n\u001b[1;32m      9\u001b[0m     \u001b[39m\"\"\"Base class for crossovers.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[39m    A crossover operation is used by :class:`~optuna.samplers.NSGAIISampler`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m        parameters (uniform crossover) is built-in into :class:`~optuna.samplers.NSGAIISampler`.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/study/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_callbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m MaxTrialsCallback\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstudy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_study_direction\u001b[39;00m \u001b[39mimport\u001b[39;00m StudyDirection\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstudy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_study_summary\u001b[39;00m \u001b[39mimport\u001b[39;00m StudySummary\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/_callbacks.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_experimental\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental_class\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_experimental\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental_func\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrial\u001b[39;00m \u001b[39mimport\u001b[39;00m FrozenTrial\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrial\u001b[39;00m \u001b[39mimport\u001b[39;00m TrialState\n\u001b[1;32m     12\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMaxTrialsCallback\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/trial/__init__.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrial\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_frozen\u001b[39;00m \u001b[39mimport\u001b[39;00m FrozenTrial\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrial\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_state\u001b[39;00m \u001b[39mimport\u001b[39;00m TrialState\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrial\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_trial\u001b[39;00m \u001b[39mimport\u001b[39;00m Trial\n\u001b[1;32m      9\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mBaseTrial\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mFixedTrial\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcreate_trial\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m ]\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/trial/_trial.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m \u001b[39mimport\u001b[39;00m distributions\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m \u001b[39mimport\u001b[39;00m logging\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m \u001b[39mimport\u001b[39;00m pruners\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_deprecated\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated_func\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39moptuna\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseDistribution\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:844\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:939\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1037\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "import warnings\n",
    "import os.path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopy\n",
    "import os\n",
    "import shutil\n",
    "import geopandas as gpd\n",
    "import catboost as cb\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "from pyproj import Geod\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, LineString\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "from catboost.utils import get_gpu_device_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "SEED = 23\n",
    "\n",
    "spatial = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "age = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "income = pd.read_csv('data/grunnkrets_income_households.csv').set_index(['grunnkrets_id', 'year']).add_prefix('income_').reset_index()\n",
    "households = pd.read_csv('data/grunnkrets_households_num_persons.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "plaace = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "busstops = pd.read_csv('data/busstops_norway.csv')\n",
    "\n",
    "train = pd.read_csv('data/stores_train.csv')\n",
    "test = pd.read_csv('data/stores_test.csv') \n",
    "\n",
    "submission = pd.read_csv('data/sample_submission.csv')  # Please do not delete this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    y_pred[y_pred < 0] = 0 + 1e-6\n",
    "    y_true[y_true < 0] = 0 + 1e-6\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "rmsle_scorer = make_scorer(lambda y, y_true: rmsle(y, y_true), greater_is_better=False)\n",
    "\n",
    "def to_categorical(df: pd.DataFrame):\n",
    "    for cat_col in df.select_dtypes(include=[object]).columns:\n",
    "        df[cat_col] = df[cat_col].astype('category')\n",
    "    return df\n",
    "\n",
    "\n",
    "def object_encoder(df: pd.DataFrame):\n",
    "    enc = OrdinalEncoder()\n",
    "    obj_cols = df.select_dtypes(include=[object]).columns\n",
    "    df[obj_cols] = enc.fit_transform(df[obj_cols])\n",
    "    return df\n",
    "\n",
    "\n",
    "def nan_to_string(df: pd.DataFrame):\n",
    "    nan = '#N/A'\n",
    "    cols = df[df.columns[df.isna().any()]].columns\n",
    "    df[cols] = df[cols].fillna(nan)\n",
    "    return df\n",
    "\n",
    "\n",
    "def meter_distance(lat1, lon1, lat2, lon2):\n",
    "    line_string = LineString([Point(lon1, lat1), Point(lon2, lat2)])\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "    return geod.geometry_length(line_string)\n",
    "\n",
    "\n",
    "def add_city_centre_dist(X: pd.DataFrame):\n",
    "    old_shape = X.shape\n",
    "\n",
    "    city_centres = X.groupby(['municipality_name'])[['lat', 'lon']].apply(lambda x: x.sum() / (x.count()))[['lat', 'lon']]\n",
    "    X = X.merge(city_centres, on=['municipality_name'], how='left', suffixes=(None, '_center'))\n",
    "    assert X.shape[0] == old_shape[0]\n",
    "\n",
    "    X.fillna(value={'lat_center': X.lat, 'lon_center': X.lon}, inplace=True)\n",
    "\n",
    "    X['dist_to_center'] = X.apply(lambda row: meter_distance(row.lat, row.lon, row.lat_center, row.lon_center), axis=1)\n",
    "    assert X.shape[0] == old_shape[0]\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def group_ages(age: pd.DataFrame, age_ranges: List[Tuple[int, int]]):\n",
    "    age_new = age[['grunnkrets_id', 'year']].drop_duplicates(subset=['grunnkrets_id'], keep='last')\n",
    "\n",
    "    for rng in age_ranges:\n",
    "        cols = [f'age_{age}' for age in range(rng[0], rng[1] + 1)]\n",
    "        rng_sum = age[cols].sum(axis=1).astype(int)\n",
    "        age_new[f'age_{rng[0]}_{rng[-1]}'] = rng_sum\n",
    "\n",
    "    age = age.drop_duplicates(subset='grunnkrets_id').drop(columns=['year', *(f'age_{age}' for age in range(0, 91))], axis=1)\n",
    "    age = age.merge(age_new.drop(columns=['year']), on='grunnkrets_id')\n",
    "\n",
    "    return age\n",
    "\n",
    "\n",
    "def only_latest_data(df: pd.DataFrame):\n",
    "    df = df.sort_values(by='year', ascending=False)\n",
    "    df = df.drop_duplicates(subset='grunnkrets_id', keep='first')\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_out_nan_heavy_rows(df: pd.DataFrame, age, age_ranges, spatial_2016, income_2016, households_2016):\n",
    "    \"\"\"Cleans out rows that have no match in the age, spatial, income or household datasets.\"\"\"\n",
    "\n",
    "    df2 = df.merge(group_ages(age, age_ranges), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(spatial_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(income_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(households_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "\n",
    "    df_cleaned = df2[\n",
    "        ~(df2.age_0_19.isna() | df2.couple_children_0_to_5_years.isna() | df2.grunnkrets_name.isna() | df2.income_all_households.isna())\n",
    "    ]\n",
    "\n",
    "    print(f'Cleaned out {len(df) - len(df_cleaned)} out of {len(df)} rows.')\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "# def add_spatial_clusters(df: pd.DataFrame):\n",
    "#     clusters = DBSCAN(eps=0.145, min_samples=100)\n",
    "#     # clusters = DBSCAN(eps=0.12, min_samples=30)\n",
    "#     cl = clusters.fit_predict(df[['lat', 'lon']].to_numpy())\n",
    "#     cl_counts = dict(zip(*np.unique(cl, return_counts=True)))\n",
    "\n",
    "#     print(len(set(cl)), 'clusters created')\n",
    "#     print('Cluster counts:', cl_counts)\n",
    "\n",
    "#     df['cluster_id'] = cl\n",
    "#     df['cluster_member_count'] = df.apply(lambda row: cl_counts[row.cluster_id], axis=1)\n",
    "\n",
    "#     X_no_outliers = df[df.cluster_id != -1]\n",
    "#     cluster_centroids = X_no_outliers.groupby('cluster_id')[['lat', 'lon']].mean()\n",
    "\n",
    "#     def closest_centroid(lat, lon):\n",
    "#         dist_series = cluster_centroids.apply(lambda row: meter_distance(lat, lon, row.lat, row.lon), axis=1)\n",
    "#         return dist_series.min()\n",
    "\n",
    "#     print('Calculating distance to closest cluster for each data point...')\n",
    "#     df['closest_cluster_centroid_dist'] = df.progress_apply(lambda row: closest_centroid(row.lat, row.lon), axis=1)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data only contains data from 2016, so for the other datasets with an age column\n",
    "we only use the values from 2016, where possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_ranges = [\n",
    "    (0, 19),\n",
    "    (20, 39),\n",
    "    (40, 59),\n",
    "    (60, 79),\n",
    "    (80, 90),\n",
    "]\n",
    "\n",
    "spatial_latest = only_latest_data(spatial)\n",
    "income_latest = only_latest_data(income)\n",
    "households_latest = only_latest_data(households)\n",
    "\n",
    "train_spatial = train.merge(spatial_latest.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "muni_avg_revenue = train_spatial.groupby(by='municipality_name', as_index=False)['revenue'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Futhermore, we noticed that a number of rows in the train and test datasets didn't have  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df: pd.DataFrame, min_val=0, max_val=100):\n",
    "    print('Length of data frame:', len(df))\n",
    "    df = df[(df.revenue > min_val) & (df.revenue < max_val)]\n",
    "    print('Length after removing extreme values and zero revenue retail stores:',  len(df))\n",
    "    return df.drop(columns=['revenue']), df.revenue\n",
    "\n",
    "\n",
    "def clean_out_nan_heavy_rows(df: pd.DataFrame):\n",
    "    \"\"\"Cleans out rows that have no match in the age, spatial, income or household datasets.\"\"\"\n",
    "\n",
    "    # df2 = df.merge(group_ages(age, age_ranges), on='grunnkrets_id', how='left')\n",
    "    df2 = df.merge(spatial_latest.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(income_latest.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(households_latest.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "\n",
    "    df_cleaned = df[\n",
    "        ~(df2.couple_children_0_to_5_years.isna() | df2.grunnkrets_name.isna() | df2.income_all_households.isna())  # | df2.age_0_19.isna() \n",
    "    ]\n",
    "\n",
    "    print(f'Cleaned out {len(df) - len(df_cleaned)} out of {len(df)} rows.')\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "train = clean_out_nan_heavy_rows(train)\n",
    "label_name = 'revenue'\n",
    "X = train.drop(columns=[label_name])\n",
    "y = train[label_name]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=.8, random_state=SEED)\n",
    "X_train, y_train = clean(pd.merge(X_train, y_train, left_index=True, right_index=True))\n",
    "\n",
    "y_train = np.log1p(y_train)\n",
    "y_val = np.log1p(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df: pd.DataFrame, data_origin: str, predictor: str = ''):\n",
    "    # Define datasets to be merged\n",
    "    age_groups_merge = group_ages(age, age_ranges)\n",
    "    spatial_merge = spatial_latest.drop(columns=['year'])\n",
    "    income_merge = income_latest.drop(columns=['year'])\n",
    "    households_merge = households_latest.drop(columns=['year'])\n",
    "    plaace_merge = plaace.drop_duplicates(subset='plaace_hierarchy_id')\n",
    "    bus_data_train_merge = gpd.read_parquet(f'derived_data/stores_bus_stops_lt_1km_{data_origin}').drop(columns=['geometry'])\n",
    "    stores_vicinity_merge = gpd.read_parquet(f'derived_data/stores_count_lt_1km_{data_origin}').drop(columns=['geometry'])\n",
    "\n",
    "    # Merge datasets\n",
    "    df = df.merge(age_groups_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(spatial_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(income_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(households_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(plaace_merge, how='left')\n",
    "    df = df.merge(bus_data_train_merge, on='store_id', how='left')\n",
    "    df = df.merge(stores_vicinity_merge, on='store_id', how='left')\n",
    "    df = add_city_centre_dist(df).drop(columns=['lon_center', 'lat_center'])\n",
    "\n",
    "    # Transformations and some post-merge cleaning\n",
    "    df.stores_count_lt_1km = np.log(df.stores_count_lt_1km)\n",
    "    df[age_groups_merge.columns] = df[age_groups_merge.columns].fillna(0)\n",
    "    \n",
    "    # Handle categories for different predictors\n",
    "    if predictor == 'xgb':\n",
    "        df = object_encoder(df)\n",
    "    elif predictor == 'cb':\n",
    "        df = nan_to_string(df)\n",
    "    elif predictor == 'lgb':\n",
    "        df = to_categorical(df)\n",
    "    else: \n",
    "        raise ValueError('Invalid predictor')\n",
    "\n",
    "    features = [\n",
    "        'store_name', \n",
    "        'mall_name', \n",
    "        'chain_name',\n",
    "        'address', \n",
    "        'lat', 'lon',\n",
    "        \n",
    "        *age_groups_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        *income_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        *households_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        'lv1_desc', 'lv2_desc', 'sales_channel_name', \n",
    "        *bus_data_train_merge.drop(columns=['store_id']).columns,\n",
    "        *stores_vicinity_merge.drop(columns=['store_id']).columns,\n",
    "        'dist_to_center'\n",
    "    ]\n",
    "\n",
    "    return df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features adapted to Catboost\n",
    "X_train_cb = generate_features(X_train, data_origin='train', predictor='cb')\n",
    "X_val_cb = generate_features(X_val, data_origin='train', predictor='cb')\n",
    "X_test_cb = generate_features(test, data_origin='test', predictor='cb')\n",
    "\n",
    "# Features adapted to LightGBM\n",
    "X_train_lgb = generate_features(X_train, data_origin='train', predictor='lgb')\n",
    "X_val_lgb = generate_features(X_val, data_origin='train', predictor='lgb')\n",
    "X_test_lgb = generate_features(test, data_origin='test', predictor='lgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing pools and parameter grid for Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cb_pools():\n",
    "    text_features = ['store_name', 'address', 'sales_channel_name'] \n",
    "    cat_features = ['mall_name', 'chain_name', 'lv1_desc', 'lv2_desc']\n",
    "\n",
    "    train_pool = cb.Pool(\n",
    "        X_train_cb,\n",
    "        y_train,\n",
    "        cat_features=cat_features,\n",
    "        text_features=text_features,\n",
    "        feature_names=list(X_train_cb)\n",
    "    )\n",
    "\n",
    "    valid_pool = cb.Pool(\n",
    "        X_val_cb,\n",
    "        y_val,\n",
    "        cat_features=cat_features,\n",
    "        text_features=text_features,\n",
    "        feature_names=list(X_train_cb)\n",
    "    )\n",
    "\n",
    "    return train_pool, valid_pool\n",
    "\n",
    "\n",
    "def get_cb_params(trial: optuna.Trial = None):\n",
    "    gpu_count = get_gpu_device_count()\n",
    "    non_tunable_cb_params = {\n",
    "        'objective': 'RMSE',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'task_type': 'GPU' if gpu_count else 'CPU', \n",
    "        'devices': f'0:{gpu_count}',\n",
    "        'random_seed': SEED\n",
    "    }\n",
    "\n",
    "    if trial is None:\n",
    "        return 'cb', non_tunable_cb_params\n",
    "    \n",
    "    tunable_params = {\n",
    "        'depth': trial.suggest_int('depth', 4, 9),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['Ordered', 'Plain']),\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 2, 6),\n",
    "        # 'iterations': trial.suggest_int('iterations', 1000, 2000),\n",
    "        # 'learning_rate': trial.suggest_categorical('learning_rate', 0.1, 0.5)\n",
    "    }\n",
    "\n",
    "    return 'cb', non_tunable_cb_params, tunable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing DMatrices and parameter grid for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lgb_dmatrices():\n",
    "    dtrain = lgb.Dataset(X_train_lgb, y_train, params={'verbose': -1}, free_raw_data=False)\n",
    "    dvalid = lgb.Dataset(X_val_lgb, y_val, params={'verbose': -1}, free_raw_data=False)\n",
    "    return dtrain, dvalid\n",
    "\n",
    "\n",
    "rmsle_scorer = make_scorer(lambda y, y_true: rmsle(y, y_true), greater_is_better=False)\n",
    "\n",
    "def get_lgb_params(trial: optuna.Trial = None):\n",
    "    non_tunable_lgb_params = {\n",
    "        'objective': 'rmse',\n",
    "        'verbose': -1,\n",
    "        'seed': 1\n",
    "    }\n",
    "\n",
    "    if trial is None:\n",
    "        return 'lgb', non_tunable_lgb_params\n",
    "\n",
    "    tunable_params = {\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'goss', 'dart']),\n",
    "    }\n",
    "\n",
    "    if tunable_params['boosting_type'] != 'goss':\n",
    "        tunable_params[\"bagging_fraction\"]: trial.suggest_float(\"bagging_fraction\", 0.4, 1.0)\n",
    "        tunable_params[\"bagging_freq\"]: trial.suggest_int(\"bagging_freq\", 1, 7)\n",
    "\n",
    "    return 'lgb', non_tunable_lgb_params, tunable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(trial: optuna.Trial, param_grid_fn: Callable) -> float:\n",
    "    model_name, non_tunable_params, tunable_params = param_grid_fn(trial)\n",
    "  \n",
    "    if model_name == 'cb':\n",
    "        if tunable_params['bootstrap_type'] == 'Bayesian': \n",
    "            tunable_params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n",
    "        elif tunable_params['bootstrap_type'] == 'Bernoulli':\n",
    "            tunable_params['subsample'] = trial.suggest_float('subsample', 0.1, 1, log=True)\n",
    "\n",
    "        cbr = cb.CatBoostRegressor(**non_tunable_params, **tunable_params) \n",
    "        train_pool, valid_pool = get_cb_pools()\n",
    "        cbr.fit(\n",
    "            train_pool,\n",
    "            eval_set=[(X_val_cb, y_val)],\n",
    "            verbose=0,\n",
    "            early_stopping_rounds=100,\n",
    "        )\n",
    "        y_pred = cbr.predict(X_val_cb)\n",
    "    \n",
    "    elif model_name == 'lgb':\n",
    "        dtrain_lgb, dvalid_lgb = get_lgb_dmatrices()\n",
    "        lgbr = lgb.train(\n",
    "            params={**non_tunable_params, **tunable_params},\n",
    "            train_set=dtrain_lgb,\n",
    "            valid_sets=dvalid_lgb,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "        y_pred = lgbr.predict(X_val_lgb)\n",
    "\n",
    "    score = rmsle(np.expm1(y_val), np.expm1(y_pred))\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_hyper_parameters(param_grid_fn: Callable, n_trials=100):\n",
    "    study = optuna.create_study(\n",
    "        study_name='hyperparam-tuning',\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), \n",
    "        direction='minimize'\n",
    "    )\n",
    "    objective_fn = lambda trial: objective(trial, param_grid_fn)\n",
    "    study.optimize(objective_fn, n_trials=n_trials, timeout=900) \n",
    "\n",
    "    print('Number of finished trials: {}'.format(len(study.trials)))\n",
    "    \n",
    "    trial = study.best_trial\n",
    "    print(f'Best trial ({trial.number}):')\n",
    "    print('Value:', trial.value)\n",
    "    print('Params:')\n",
    "    print(trial.params)\n",
    "\n",
    "    return param_grid_fn()[1], trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_tunable_cb_params, tuned_params = get_hyper_parameters(get_cb_params, n_trials=30)\n",
    "train_pool, valid_pool = get_cb_pools()\n",
    "cbm = cb.CatBoostRegressor(**non_tunable_cb_params, **tuned_params, iterations=1000) \n",
    "cbm.fit(train_pool, eval_set=valid_pool, verbose=50, plot=True, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_tunable_lgb_params, tunable_lgb_params = get_hyper_parameters(get_lgb_params, n_trials=400)\n",
    "dtrain_lgb, dvalid_lgb = get_lgb_dmatrices()\n",
    "lgbm = lgb.train(\n",
    "    params={**non_tunable_lgb_params, **tunable_lgb_params},\n",
    "    train_set=dtrain_lgb,\n",
    "    valid_sets=dvalid_lgb,\n",
    "    verbose_eval=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_lgb, dvalid_lgb = get_lgb_dmatrices()\n",
    "lgbm = lgb.train(\n",
    "    params={**get_lgb_params()[1], **{'lambda_l1': 0.026873437304227192, 'lambda_l2': 9.270495297846024, 'num_leaves': 7, 'feature_fraction': 0.8633788253368204, 'min_child_samples': 81, 'boosting_type': 'gbdt'}},\n",
    "    train_set=dtrain_lgb,\n",
    "    valid_sets=dvalid_lgb,\n",
    "    verbose_eval=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost validation prediction\n",
    "y_pred_val_cb = np.expm1(cbm.predict(X_val_cb))\n",
    "print('LightGBM validation score:', rmsle(np.expm1(y_val), y_pred_val_cb))\n",
    "\n",
    "# Catboost validation prediction\n",
    "y_val_pred_lgb = np.expm1(lgbm.predict(X_val_lgb))\n",
    "print('LightGBM validation score:', rmsle(np.expm1(y_val), y_val_pred_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_cb = np.expm1(cbm.predict(X_test_cb))\n",
    "y_pred_test_lgb = np.expm1(lgbm.predict(X_test_lgb))\n",
    "\n",
    "test_stack = np.array([y_pred_test_cb, y_pred_test_lgb])\n",
    "stack_test_avg = np.mean(test_stack, axis=0)\n",
    "\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['predicted'] = stack_test_avg\n",
    "submission.to_csv('submissions/cb_lgb_automl_stacked2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1beb4c19f9c9850d9088b15a4a1b3063555a573f9fb8d1ae667cbe7a8ada917e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
