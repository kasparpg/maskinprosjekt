{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Install required packages\n",
    "\n",
    "We ran into some issues with some of the geo related packages, such as geopandas, on certain machines. If the code does not run on your machine, you can comment out the lines commented with \"# GEO PROBLEM HERE\", and it will hopefully allow the code to run. This may of course impact the final score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import warnings\n",
    "import os.path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopy\n",
    "import os\n",
    "import shutil\n",
    "import geopandas as gpd\n",
    "import catboost as cb\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "from pyproj import Geod\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, LineString\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "from catboost.utils import get_gpu_device_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "SEED = 23\n",
    "\n",
    "spatial = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "age = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "income = pd.read_csv('data/grunnkrets_income_households.csv').set_index(['grunnkrets_id', 'year']).add_prefix('income_').reset_index()\n",
    "households = pd.read_csv('data/grunnkrets_households_num_persons.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "plaace = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "busstops = pd.read_csv('data/busstops_norway.csv')\n",
    "\n",
    "train = pd.read_csv('data/stores_train.csv')\n",
    "test = pd.read_csv('data/stores_test.csv') \n",
    "\n",
    "submission = pd.read_csv('data/sample_submission.csv')  # Please do not delete this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(df: pd.DataFrame):\n",
    "    for cat_col in df.select_dtypes(include=[object]).columns:\n",
    "        df[cat_col] = df[cat_col].astype('category')\n",
    "    return df\n",
    "\n",
    "\n",
    "def object_encoder(df: pd.DataFrame):\n",
    "    enc = OrdinalEncoder()\n",
    "    obj_cols = df.select_dtypes(include=[object]).columns\n",
    "    df[obj_cols] = enc.fit_transform(df[obj_cols])\n",
    "    return df\n",
    "\n",
    "\n",
    "def nan_to_string(df: pd.DataFrame):\n",
    "    nan = '#N/A'\n",
    "    cols = df[df.columns[df.isna().any()]].columns\n",
    "    df[cols] = df[cols].fillna(nan)\n",
    "    return df\n",
    "\n",
    "\n",
    "def meter_distance(lat1, lon1, lat2, lon2):\n",
    "    line_string = LineString([Point(lon1, lat1), Point(lon2, lat2)])\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "    return geod.geometry_length(line_string)\n",
    "\n",
    "\n",
    "def add_city_centre_dist(X: pd.DataFrame):\n",
    "    old_shape = X.shape\n",
    "\n",
    "    city_centres = X.groupby(['municipality_name'])[['lat', 'lon']].apply(lambda x: x.sum() / (x.count()))[['lat', 'lon']]\n",
    "    X = X.merge(city_centres, on=['municipality_name'], how='left', suffixes=(None, '_center'))\n",
    "    assert X.shape[0] == old_shape[0]\n",
    "\n",
    "    X.fillna(value={'lat_center': X.lat, 'lon_center': X.lon}, inplace=True)\n",
    "\n",
    "    X['dist_to_center'] = X.apply(lambda row: meter_distance(row.lat, row.lon, row.lat_center, row.lon_center), axis=1)\n",
    "    assert X.shape[0] == old_shape[0]\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def group_ages(age: pd.DataFrame, age_ranges: List[Tuple[int, int]]):\n",
    "    age_new = age[['grunnkrets_id', 'year']].drop_duplicates(subset=['grunnkrets_id'], keep='last')\n",
    "\n",
    "    for rng in age_ranges:\n",
    "        cols = [f'age_{age}' for age in range(rng[0], rng[1] + 1)]\n",
    "        rng_sum = age[cols].sum(axis=1).astype(int)\n",
    "        age_new[f'age_{rng[0]}_{rng[-1]}'] = rng_sum\n",
    "\n",
    "    age = age.drop_duplicates(subset='grunnkrets_id').drop(columns=['year', *(f'age_{age}' for age in range(0, 91))], axis=1)\n",
    "    age = age.merge(age_new.drop(columns=['year']), on='grunnkrets_id')\n",
    "\n",
    "    return age\n",
    "\n",
    "\n",
    "def only_2016_data(df: pd.DataFrame):\n",
    "    df = df.sort_values(by='year', ascending=False)\n",
    "    df = df.drop_duplicates(subset='grunnkrets_id', keep='first')\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_out_nan_heavy_rows(df: pd.DataFrame, age, age_ranges, spatial_2016, income_2016, households_2016):\n",
    "    \"\"\"Cleans out rows that have no match in the age, spatial, income or household datasets.\"\"\"\n",
    "\n",
    "    df2 = df.merge(group_ages(age, age_ranges), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(spatial_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(income_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(households_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "\n",
    "    df_cleaned = df2[\n",
    "        ~(df2.age_0_19.isna() | df2.couple_children_0_to_5_years.isna() | df2.grunnkrets_name.isna() | df2.income_all_households.isna())\n",
    "    ]\n",
    "\n",
    "    print(f'Cleaned out {len(df) - len(df_cleaned)} out of {len(df)} rows.')\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "# def add_spatial_clusters(df: pd.DataFrame):\n",
    "#     clusters = DBSCAN(eps=0.145, min_samples=100)\n",
    "#     # clusters = DBSCAN(eps=0.12, min_samples=30)\n",
    "#     cl = clusters.fit_predict(df[['lat', 'lon']].to_numpy())\n",
    "#     cl_counts = dict(zip(*np.unique(cl, return_counts=True)))\n",
    "\n",
    "#     print(len(set(cl)), 'clusters created')\n",
    "#     print('Cluster counts:', cl_counts)\n",
    "\n",
    "#     df['cluster_id'] = cl\n",
    "#     df['cluster_member_count'] = df.apply(lambda row: cl_counts[row.cluster_id], axis=1)\n",
    "\n",
    "#     X_no_outliers = df[df.cluster_id != -1]\n",
    "#     cluster_centroids = X_no_outliers.groupby('cluster_id')[['lat', 'lon']].mean()\n",
    "\n",
    "#     def closest_centroid(lat, lon):\n",
    "#         dist_series = cluster_centroids.apply(lambda row: meter_distance(lat, lon, row.lat, row.lon), axis=1)\n",
    "#         return dist_series.min()\n",
    "\n",
    "#     print('Calculating distance to closest cluster for each data point...')\n",
    "#     df['closest_cluster_centroid_dist'] = df.progress_apply(lambda row: closest_centroid(row.lat, row.lon), axis=1)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data only contains data from 2016, so for the other datasets with an age column\n",
    "we only use the values from 2016, where possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_ranges = [\n",
    "    (0, 19),\n",
    "    (20, 39),\n",
    "    (40, 59),\n",
    "    (60, 79),\n",
    "    (80, 90),\n",
    "]\n",
    "\n",
    "spatial_2016 = only_2016_data(spatial)\n",
    "income_2016 = only_2016_data(income)\n",
    "households_2016 = only_2016_data(households)\n",
    "\n",
    "train_spatial = train.merge(spatial_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "muni_avg_revenue = train_spatial.groupby(by='municipality_name', as_index=False)['revenue'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Futhermore, we noticed that a number of rows in the train and test datasets didn't have  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df: pd.DataFrame, min_val=0, max_val=100):\n",
    "    print('Length of data frame:', len(df))\n",
    "    df = df[(df.revenue > min_val) & (df.revenue < max_val)]\n",
    "    print('Length after removing extreme values and zero revenue retail stores:',  len(df))\n",
    "    return df.drop(columns=['revenue']), df.revenue\n",
    "\n",
    "\n",
    "def clean_out_nan_heavy_rows(df: pd.DataFrame):\n",
    "    \"\"\"Cleans out rows that have no match in the age, spatial, income or household datasets.\"\"\"\n",
    "\n",
    "    # df2 = df.merge(group_ages(age, age_ranges), on='grunnkrets_id', how='left')\n",
    "    df2 = df.merge(spatial_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(income_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(households_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "\n",
    "    df_cleaned = df[\n",
    "        ~(df2.couple_children_0_to_5_years.isna() | df2.grunnkrets_name.isna() | df2.income_all_households.isna())  # | df2.age_0_19.isna() \n",
    "    ]\n",
    "\n",
    "    print(f'Cleaned out {len(df) - len(df_cleaned)} out of {len(df)} rows.')\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "train = clean_out_nan_heavy_rows(train)\n",
    "label_name = 'revenue'\n",
    "X = train.drop(columns=[label_name])\n",
    "y = train[label_name]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=.8, random_state=SEED)\n",
    "X_train, y_train = clean(pd.merge(X_train, y_train, left_index=True, right_index=True))\n",
    "\n",
    "y_train = np.log1p(y_train)\n",
    "y_val = np.log1p(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df: pd.DataFrame, data_origin: str, predictor: str = ''):\n",
    "    # Define datasets to be merged\n",
    "    age_groups_merge = group_ages(age, age_ranges)\n",
    "    spatial_merge = spatial_2016.drop(columns=['year'])\n",
    "    income_merge = income_2016.drop(columns=['year'])\n",
    "    households_merge = households_2016.drop(columns=['year'])\n",
    "    plaace_merge = plaace.drop_duplicates(subset='plaace_hierarchy_id')\n",
    "    bus_data_train_merge = gpd.read_parquet(f'derived_data/stores_bus_stops_lt_1km_{data_origin}').drop(columns=['geometry'])\n",
    "    stores_vicinity_merge = gpd.read_parquet(f'derived_data/stores_count_lt_1km_{data_origin}').drop(columns=['geometry'])\n",
    "\n",
    "    # Merge datasets\n",
    "    df = df.merge(age_groups_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(spatial_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(income_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(households_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(plaace_merge, how='left')\n",
    "    df = df.merge(bus_data_train_merge, on='store_id', how='left')\n",
    "    df = df.merge(stores_vicinity_merge, on='store_id', how='left')\n",
    "    df = add_city_centre_dist(df).drop(columns=['lon_center', 'lat_center'])\n",
    "\n",
    "    # Transformations and some post-merge cleaning\n",
    "    df.stores_count_lt_1km = np.log(df.stores_count_lt_1km)\n",
    "    df[age_groups_merge.columns] = df[age_groups_merge.columns].fillna(0)\n",
    "    \n",
    "\n",
    "    # Handle categories for different predictors\n",
    "    if predictor == 'xgb':\n",
    "        df = object_encoder(df)\n",
    "    elif predictor == 'cb':\n",
    "        df = nan_to_string(df)\n",
    "    elif predictor == 'lgb':\n",
    "        df = to_categorical(df)\n",
    "    else: \n",
    "        raise ValueError('Invalid predictor')\n",
    "\n",
    "    features = [\n",
    "        'store_name', \n",
    "        'mall_name', \n",
    "        'chain_name',\n",
    "        'address', \n",
    "        'lat', 'lon',\n",
    "        \n",
    "        *age_groups_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        *income_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        *households_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        'lv1_desc', 'lv2_desc', 'sales_channel_name', \n",
    "        *bus_data_train_merge.drop(columns=['store_id']).columns,\n",
    "        *stores_vicinity_merge.drop(columns=['store_id']).columns,\n",
    "        'dist_to_center'\n",
    "    ]\n",
    "\n",
    "    return df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features adapted to Catboost\n",
    "X_train_cb = generate_features(X_train, data_origin='train', predictor='cb')\n",
    "X_val_cb = generate_features(X_val, data_origin='train', predictor='cb')\n",
    "X_test_cb = generate_features(test, data_origin='test', predictor='cb')\n",
    "\n",
    "# Features adapted to LightGBM\n",
    "X_train_lgb = generate_features(X_train, data_origin='train', predictor='lgb')\n",
    "X_val_lgb = generate_features(X_val, data_origin='train', predictor='lgb')\n",
    "X_test_lgb = generate_features(test, data_origin='test', predictor='lgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing pools and parameter grid for Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cb_pools():\n",
    "    text_features = ['store_name', 'address', 'sales_channel_name'] \n",
    "    cat_features = ['mall_name', 'chain_name', 'lv1_desc', 'lv2_desc']\n",
    "\n",
    "    train_pool = cb.Pool(\n",
    "        X_train_cb,\n",
    "        y_train,\n",
    "        cat_features=cat_features,\n",
    "        text_features=text_features,\n",
    "        feature_names=list(X_train_cb)\n",
    "    )\n",
    "\n",
    "    valid_pool = cb.Pool(\n",
    "        X_val_cb,\n",
    "        y_val,\n",
    "        cat_features=cat_features,\n",
    "        text_features=text_features,\n",
    "        feature_names=list(X_train_cb)\n",
    "    )\n",
    "\n",
    "    return train_pool, valid_pool\n",
    "\n",
    "\n",
    "def get_cb_params(trial: optuna.Trial = None):\n",
    "    gpu_count = get_gpu_device_count()\n",
    "    non_tunable_cb_params = {\n",
    "        'objective': 'RMSE',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'task_type': 'GPU' if gpu_count else 'CPU', \n",
    "        'devices': f'0:{gpu_count}',\n",
    "        'random_seed': SEED\n",
    "    }\n",
    "\n",
    "    if trial is None:\n",
    "        return 'cb', non_tunable_cb_params\n",
    "    \n",
    "    tunable_params = {\n",
    "        'depth': trial.suggest_int('depth', 4, 9),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['Ordered', 'Plain']),\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 2, 6),\n",
    "        # 'iterations': trial.suggest_int('iterations', 1000, 2000),\n",
    "        # 'learning_rate': trial.suggest_categorical('learning_rate', 0.1, 0.5)\n",
    "    }\n",
    "\n",
    "    return 'cb', non_tunable_cb_params, tunable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing DMatrices and parameter grid for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lgb_dmatrices():\n",
    "    dtrain = lgb.Dataset(X_train_lgb, y_train, params={'verbose': -1}, free_raw_data=False)\n",
    "    dvalid = lgb.Dataset(X_val_lgb, y_val, params={'verbose': -1}, free_raw_data=False)\n",
    "    return dtrain, dvalid\n",
    "\n",
    "\n",
    "def get_lgb_params(trial: optuna.Trial = None):\n",
    "    non_tunable_lgb_params = {\n",
    "        'objective': 'rmse',\n",
    "        'verbose': -1,\n",
    "        'seed': 1\n",
    "    }\n",
    "\n",
    "    if trial is None:\n",
    "        return 'lgb', non_tunable_lgb_params\n",
    "\n",
    "    tunable_params = {\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'goss', 'dart']),\n",
    "    }\n",
    "\n",
    "    if tunable_params['boosting_type'] != 'goss':\n",
    "        tunable_params[\"bagging_fraction\"]: trial.suggest_float(\"bagging_fraction\", 0.4, 1.0)\n",
    "        tunable_params[\"bagging_freq\"]: trial.suggest_int(\"bagging_freq\", 1, 7)\n",
    "\n",
    "    return 'lgb', non_tunable_lgb_params, tunable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(trial: optuna.Trial, param_grid_fn: Callable) -> float:\n",
    "    model_name, non_tunable_params, tunable_params = param_grid_fn(trial)\n",
    "  \n",
    "    if model_name == 'cb':\n",
    "        if tunable_params['bootstrap_type'] == 'Bayesian': \n",
    "            tunable_params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n",
    "        elif tunable_params['bootstrap_type'] == 'Bernoulli':\n",
    "            tunable_params['subsample'] = trial.suggest_float('subsample', 0.1, 1, log=True)\n",
    "\n",
    "        cbr = cb.CatBoostRegressor(**non_tunable_params, **tunable_params) \n",
    "        train_pool, valid_pool = get_cb_pools()\n",
    "        cbr.fit(\n",
    "            train_pool,\n",
    "            eval_set=[(X_val_cb, y_val)],\n",
    "            verbose=0,\n",
    "            early_stopping_rounds=100,\n",
    "        )\n",
    "        y_pred = cbr.predict(X_val_cb)\n",
    "    \n",
    "    elif model_name == 'lgb':\n",
    "        dtrain_lgb, dvalid_lgb = get_lgb_dmatrices()\n",
    "        lgbr = lgb.train(\n",
    "            params={**non_tunable_params, **tunable_params},\n",
    "            train_set=dtrain_lgb,\n",
    "            valid_sets=dvalid_lgb,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "        y_pred = lgbr.predict(X_val_lgb)\n",
    "\n",
    "    score = rmsle(np.expm1(y_val), np.expm1(y_pred))\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_hyper_parameters(param_grid_fn: Callable, n_trials=100):\n",
    "    study = optuna.create_study(\n",
    "        study_name='hyperparam-tuning',\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), \n",
    "        direction='minimize'\n",
    "    )\n",
    "    objective_fn = lambda trial: objective(trial, param_grid_fn)\n",
    "    study.optimize(objective_fn, n_trials=n_trials, timeout=900) \n",
    "\n",
    "    print('Number of finished trials: {}'.format(len(study.trials)))\n",
    "    \n",
    "    trial = study.best_trial\n",
    "    print(f'Best trial ({trial.number}):')\n",
    "    print('Value:', trial.value)\n",
    "    print('Params:')\n",
    "    print(trial.params)\n",
    "\n",
    "    return param_grid_fn()[1], trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_tunable_cb_params, tuned_params = get_hyper_parameters(get_cb_params, n_trials=30)\n",
    "train_pool, valid_pool = get_cb_pools()\n",
    "cbm = cb.CatBoostRegressor(**non_tunable_cb_params, **tuned_params, iterations=1000) \n",
    "cbm.fit(train_pool, eval_set=valid_pool, verbose=50, plot=True, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_tunable_lgb_params, tunable_lgb_params = get_hyper_parameters(get_lgb_params, n_trials=400)\n",
    "\n",
    "dtrain_lgb, dvalid_lgb = get_lgb_dmatrices()\n",
    "lgbm = lgb.train(\n",
    "    params={**non_tunable_lgb_params, **tunable_lgb_params},\n",
    "    train_set=dtrain_lgb,\n",
    "    valid_sets=dvalid_lgb,\n",
    "    verbose_eval=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_lgb, dvalid_lgb = get_lgb_dmatrices()\n",
    "lgbm = lgb.train(\n",
    "    params={**get_lgb_params()[1], **{'lambda_l1': 0.026873437304227192, 'lambda_l2': 9.270495297846024, 'num_leaves': 7, 'feature_fraction': 0.8633788253368204, 'min_child_samples': 81, 'boosting_type': 'gbdt'}},\n",
    "    train_set=dtrain_lgb,\n",
    "    valid_sets=dvalid_lgb,\n",
    "    verbose_eval=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost validation prediction\n",
    "y_pred_val_cb = np.expm1(cbm.predict(X_val_cb))\n",
    "print('LightGBM validation score:', rmsle(np.expm1(y_val), y_pred_val_cb))\n",
    "\n",
    "# Catboost validation prediction\n",
    "y_val_pred_lgb = np.expm1(lgbm.predict(X_val_lgb))\n",
    "print('LightGBM validation score:', rmsle(np.expm1(y_val), y_val_pred_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_cb = np.expm1(cbm.predict(X_test_cb))\n",
    "y_pred_test_lgb = np.expm1(lgbm.predict(X_test_lgb))\n",
    "\n",
    "test_stack = np.array([y_pred_test_cb, y_pred_test_lgb])\n",
    "stack_test_avg = np.mean(test_stack, axis=0)\n",
    "\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['predicted'] = stack_test_avg\n",
    "submission.to_csv('submissions/cb_lgb_automl_stacked2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1beb4c19f9c9850d9088b15a4a1b3063555a573f9fb8d1ae667cbe7a8ada917e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
