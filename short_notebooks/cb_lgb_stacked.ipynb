{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Install required packages\n",
    "\n",
    "We ran into some issues with some of the geo related packages, such as geopandas, on certain machines. If the code does not run on your machine, you can comment out the lines commented with \"# GEO PROBLEM HERE\", and it will hopefully allow the code to run. This may of course impact the final score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asttokens==2.1.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: attrs==22.1.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 2)) (22.1.0)\n",
      "Requirement already satisfied: autopep8==2.0.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: catboost==1.1.1 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 5)) (1.1.1)\n",
      "Requirement already satisfied: certifi==2022.9.24 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 6)) (2022.9.24)\n",
      "Requirement already satisfied: click==8.1.3 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 7)) (8.1.3)\n",
      "Requirement already satisfied: click-plugins==1.1.1 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 8)) (1.1.1)\n",
      "Requirement already satisfied: cligj==0.7.2 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 9)) (0.7.2)\n",
      "Requirement already satisfied: colorama==0.4.6 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: contourpy==1.0.6 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 11)) (1.0.6)\n",
      "Requirement already satisfied: cycler==0.11.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 12)) (0.11.0)\n",
      "Requirement already satisfied: Cython==0.29.28 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 13)) (0.29.28)\n",
      "Requirement already satisfied: debugpy==1.6.3 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 14)) (1.6.3)\n",
      "Requirement already satisfied: decorator==5.1.1 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 15)) (5.1.1)\n",
      "Requirement already satisfied: entrypoints==0.4 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 16)) (0.4)\n",
      "Requirement already satisfied: executing==1.2.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 17)) (1.2.0)\n",
      "Requirement already satisfied: Fiona==1.8.22 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 18)) (1.8.22)\n",
      "Requirement already satisfied: fonttools==4.38.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 19)) (4.38.0)\n",
      "Requirement already satisfied: gensim==4.2.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 20)) (4.2.0)\n",
      "Requirement already satisfied: geographiclib==1.52 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 21)) (1.52)\n",
      "Requirement already satisfied: geopandas==0.12.1 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 22)) (0.12.1)\n",
      "Requirement already satisfied: geopy==2.2.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 23)) (2.2.0)\n",
      "Requirement already satisfied: graphviz==0.20.1 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 24)) (0.20.1)\n",
      "Requirement already satisfied: ipykernel==6.17.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 25)) (6.17.0)\n",
      "Requirement already satisfied: ipython==8.6.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 26)) (8.6.0)\n",
      "Requirement already satisfied: jedi==0.18.1 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 27)) (0.18.1)\n",
      "Requirement already satisfied: joblib==1.2.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 28)) (1.2.0)\n",
      "Requirement already satisfied: jupyter_client==7.4.4 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 29)) (7.4.4)\n",
      "Requirement already satisfied: jupyter_core==4.11.2 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 30)) (4.11.2)\n",
      "Requirement already satisfied: kiwisolver==1.4.4 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 31)) (1.4.4)\n",
      "Requirement already satisfied: matplotlib==3.6.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 32)) (3.6.0)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 33)) (0.1.6)\n",
      "Requirement already satisfied: munch==2.5.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 34)) (2.5.0)\n",
      "Requirement already satisfied: nest-asyncio==1.5.6 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 35)) (1.5.6)\n",
      "Requirement already satisfied: numpy==1.23.4 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 36)) (1.23.4)\n",
      "Requirement already satisfied: packaging==21.3 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 37)) (21.3)\n",
      "Requirement already satisfied: pandas==1.5.1 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 38)) (1.5.1)\n",
      "Requirement already satisfied: parso==0.8.3 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 39)) (0.8.3)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 40)) (0.7.5)\n",
      "Requirement already satisfied: Pillow==9.3.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 41)) (9.3.0)\n",
      "Requirement already satisfied: plotly==5.11.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 42)) (5.11.0)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.31 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 43)) (3.0.31)\n",
      "Requirement already satisfied: psutil==5.9.3 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 44)) (5.9.3)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 45)) (0.2.2)\n",
      "Requirement already satisfied: pyarrow==10.0.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 46)) (10.0.0)\n",
      "Requirement already satisfied: pycodestyle==2.9.1 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 47)) (2.9.1)\n",
      "Requirement already satisfied: pygeos==0.13 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 48)) (0.13)\n",
      "Requirement already satisfied: Pygments==2.13.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 49)) (2.13.0)\n",
      "Requirement already satisfied: pyparsing==3.0.9 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 50)) (3.0.9)\n",
      "Requirement already satisfied: pyproj==3.4.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 51)) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 52)) (2.8.2)\n",
      "Requirement already satisfied: pytz==2022.6 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 53)) (2022.6)\n",
      "Requirement already satisfied: pyzmq==24.0.1 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 54)) (24.0.1)\n",
      "Requirement already satisfied: scikit-learn==1.1.3 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 55)) (1.1.3)\n",
      "Requirement already satisfied: scipy==1.9.3 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 56)) (1.9.3)\n",
      "Requirement already satisfied: seaborn==0.12.1 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 57)) (0.12.1)\n",
      "Requirement already satisfied: Shapely==1.8.5.post1 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 58)) (1.8.5.post1)\n",
      "Requirement already satisfied: six==1.16.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 59)) (1.16.0)\n",
      "Requirement already satisfied: sklearn==0.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 60)) (0.0)\n",
      "Requirement already satisfied: smart-open==6.2.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 61)) (6.2.0)\n",
      "Requirement already satisfied: stack-data==0.6.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 62)) (0.6.0)\n",
      "Requirement already satisfied: tenacity==8.1.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 63)) (8.1.0)\n",
      "Requirement already satisfied: threadpoolctl==3.1.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 64)) (3.1.0)\n",
      "Requirement already satisfied: tomli==2.0.1 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 65)) (2.0.1)\n",
      "Requirement already satisfied: tornado==6.2 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 66)) (6.2)\n",
      "Requirement already satisfied: tqdm==4.64.1 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 67)) (4.64.1)\n",
      "Requirement already satisfied: traitlets==5.5.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 68)) (5.5.0)\n",
      "Requirement already satisfied: wcwidth==0.2.5 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 69)) (0.2.5)\n",
      "Requirement already satisfied: xgboost==1.7.0 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from -r ../requirements.txt (line 70)) (1.7.0)\n",
      "Requirement already satisfied: setuptools in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from Fiona==1.8.22->-r ../requirements.txt (line 18)) (44.0.0)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from ipython==8.6.0->-r ../requirements.txt (line 26)) (4.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython==8.6.0->-r ../requirements.txt (line 26)) (0.7.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/grunnkrets_norway_stripped.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m pd\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mmode\u001b[39m.\u001b[39mchained_assignment \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# default='warn'\u001b[39;00m\n\u001b[1;32m     34\u001b[0m SEED \u001b[39m=\u001b[39m \u001b[39m23\u001b[39m\n\u001b[0;32m---> 36\u001b[0m spatial \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mdata/grunnkrets_norway_stripped.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     37\u001b[0m age \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdata/grunnkrets_age_distribution.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     38\u001b[0m income \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdata/grunnkrets_income_households.csv\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mset_index([\u001b[39m'\u001b[39m\u001b[39mgrunnkrets_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39madd_prefix(\u001b[39m'\u001b[39m\u001b[39mincome_\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreset_index()\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/grunnkrets_norway_stripped.csv'"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "import warnings\n",
    "import os.path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopy\n",
    "import os\n",
    "import shutil\n",
    "import geopandas as gpd\n",
    "import catboost as cb\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "from pyproj import Geod\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, LineString\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "from catboost.utils import get_gpu_device_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "SEED = 23\n",
    "\n",
    "spatial = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "age = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "income = pd.read_csv('data/grunnkrets_income_households.csv').set_index(['grunnkrets_id', 'year']).add_prefix('income_').reset_index()\n",
    "households = pd.read_csv('data/grunnkrets_households_num_persons.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "plaace = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "busstops = pd.read_csv('data/busstops_norway.csv')\n",
    "\n",
    "train = pd.read_csv('data/stores_train.csv')\n",
    "test = pd.read_csv('data/stores_test.csv') \n",
    "\n",
    "submission = pd.read_csv('data/sample_submission.csv')  # Please do not delete this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred):\n",
    "    y_pred[y_pred < 0] = 0 + 1e-6\n",
    "    y_true[y_true < 0] = 0 + 1e-6\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "rmsle_scorer = make_scorer(lambda y, y_true: rmsle(y, y_true), greater_is_better=False)\n",
    "\n",
    "def to_categorical(df: pd.DataFrame):\n",
    "    for cat_col in df.select_dtypes(include=[object]).columns:\n",
    "        df[cat_col] = df[cat_col].astype('category')\n",
    "    return df\n",
    "\n",
    "\n",
    "def object_encoder(df: pd.DataFrame):\n",
    "    enc = OrdinalEncoder()\n",
    "    obj_cols = df.select_dtypes(include=[object]).columns\n",
    "    df[obj_cols] = enc.fit_transform(df[obj_cols])\n",
    "    return df\n",
    "\n",
    "\n",
    "def nan_to_string(df: pd.DataFrame):\n",
    "    nan = '#N/A'\n",
    "    cols = df[df.columns[df.isna().any()]].columns\n",
    "    df[cols] = df[cols].fillna(nan)\n",
    "    return df\n",
    "\n",
    "\n",
    "def meter_distance(lat1, lon1, lat2, lon2):\n",
    "    line_string = LineString([Point(lon1, lat1), Point(lon2, lat2)])\n",
    "    geod = Geod(ellps=\"WGS84\")\n",
    "    return geod.geometry_length(line_string)\n",
    "\n",
    "\n",
    "def add_city_centre_dist(X: pd.DataFrame):\n",
    "    old_shape = X.shape\n",
    "\n",
    "    city_centres = X.groupby(['municipality_name'])[['lat', 'lon']].apply(lambda x: x.sum() / (x.count()))[['lat', 'lon']]\n",
    "    X = X.merge(city_centres, on=['municipality_name'], how='left', suffixes=(None, '_center'))\n",
    "    assert X.shape[0] == old_shape[0]\n",
    "\n",
    "    X.fillna(value={'lat_center': X.lat, 'lon_center': X.lon}, inplace=True)\n",
    "\n",
    "    X['dist_to_center'] = X.apply(lambda row: meter_distance(row.lat, row.lon, row.lat_center, row.lon_center), axis=1)\n",
    "    assert X.shape[0] == old_shape[0]\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def group_ages(age: pd.DataFrame, age_ranges: List[Tuple[int, int]]):\n",
    "    age_new = age[['grunnkrets_id', 'year']].drop_duplicates(subset=['grunnkrets_id'], keep='last')\n",
    "\n",
    "    for rng in age_ranges:\n",
    "        cols = [f'age_{age}' for age in range(rng[0], rng[1] + 1)]\n",
    "        rng_sum = age[cols].sum(axis=1).astype(int)\n",
    "        age_new[f'age_{rng[0]}_{rng[-1]}'] = rng_sum\n",
    "\n",
    "    age = age.drop_duplicates(subset='grunnkrets_id').drop(columns=['year', *(f'age_{age}' for age in range(0, 91))], axis=1)\n",
    "    age = age.merge(age_new.drop(columns=['year']), on='grunnkrets_id')\n",
    "\n",
    "    return age\n",
    "\n",
    "\n",
    "def only_latest_data(df: pd.DataFrame):\n",
    "    df = df.sort_values(by='year', ascending=False)\n",
    "    df = df.drop_duplicates(subset='grunnkrets_id', keep='first')\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_out_nan_heavy_rows(df: pd.DataFrame, age, age_ranges, spatial_2016, income_2016, households_2016):\n",
    "    \"\"\"Cleans out rows that have no match in the age, spatial, income or household datasets.\"\"\"\n",
    "\n",
    "    df2 = df.merge(group_ages(age, age_ranges), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(spatial_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(income_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(households_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "\n",
    "    df_cleaned = df2[\n",
    "        ~(df2.age_0_19.isna() | df2.couple_children_0_to_5_years.isna() | df2.grunnkrets_name.isna() | df2.income_all_households.isna())\n",
    "    ]\n",
    "\n",
    "    print(f'Cleaned out {len(df) - len(df_cleaned)} out of {len(df)} rows.')\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "# def add_spatial_clusters(df: pd.DataFrame):\n",
    "#     clusters = DBSCAN(eps=0.145, min_samples=100)\n",
    "#     # clusters = DBSCAN(eps=0.12, min_samples=30)\n",
    "#     cl = clusters.fit_predict(df[['lat', 'lon']].to_numpy())\n",
    "#     cl_counts = dict(zip(*np.unique(cl, return_counts=True)))\n",
    "\n",
    "#     print(len(set(cl)), 'clusters created')\n",
    "#     print('Cluster counts:', cl_counts)\n",
    "\n",
    "#     df['cluster_id'] = cl\n",
    "#     df['cluster_member_count'] = df.apply(lambda row: cl_counts[row.cluster_id], axis=1)\n",
    "\n",
    "#     X_no_outliers = df[df.cluster_id != -1]\n",
    "#     cluster_centroids = X_no_outliers.groupby('cluster_id')[['lat', 'lon']].mean()\n",
    "\n",
    "#     def closest_centroid(lat, lon):\n",
    "#         dist_series = cluster_centroids.apply(lambda row: meter_distance(lat, lon, row.lat, row.lon), axis=1)\n",
    "#         return dist_series.min()\n",
    "\n",
    "#     print('Calculating distance to closest cluster for each data point...')\n",
    "#     df['closest_cluster_centroid_dist'] = df.progress_apply(lambda row: closest_centroid(row.lat, row.lon), axis=1)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data only contains data from 2016, so for the other datasets with an age column\n",
    "we only use the values from 2016, where possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_ranges = [\n",
    "    (0, 19),\n",
    "    (20, 39),\n",
    "    (40, 59),\n",
    "    (60, 79),\n",
    "    (80, 90),\n",
    "]\n",
    "\n",
    "spatial_latest = only_latest_data(spatial)\n",
    "income_latest = only_latest_data(income)\n",
    "households_latest = only_latest_data(households)\n",
    "\n",
    "train_spatial = train.merge(spatial_latest.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "muni_avg_revenue = train_spatial.groupby(by='municipality_name', as_index=False)['revenue'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Futhermore, we noticed that a number of rows in the train and test datasets didn't have  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df: pd.DataFrame, min_val=0, max_val=100):\n",
    "    print('Length of data frame:', len(df))\n",
    "    df = df[(df.revenue > min_val) & (df.revenue < max_val)]\n",
    "    print('Length after removing extreme values and zero revenue retail stores:',  len(df))\n",
    "    return df.drop(columns=['revenue']), df.revenue\n",
    "\n",
    "\n",
    "def clean_out_nan_heavy_rows(df: pd.DataFrame):\n",
    "    \"\"\"Cleans out rows that have no match in the age, spatial, income or household datasets.\"\"\"\n",
    "\n",
    "    # df2 = df.merge(group_ages(age, age_ranges), on='grunnkrets_id', how='left')\n",
    "    df2 = df.merge(spatial_latest.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(income_latest.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(households_latest.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "\n",
    "    df_cleaned = df[\n",
    "        ~(df2.couple_children_0_to_5_years.isna() | df2.grunnkrets_name.isna() | df2.income_all_households.isna())  # | df2.age_0_19.isna() \n",
    "    ]\n",
    "\n",
    "    print(f'Cleaned out {len(df) - len(df_cleaned)} out of {len(df)} rows.')\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "train = clean_out_nan_heavy_rows(train)\n",
    "label_name = 'revenue'\n",
    "X = train.drop(columns=[label_name])\n",
    "y = train[label_name]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=.8, random_state=SEED)\n",
    "X_train, y_train = clean(pd.merge(X_train, y_train, left_index=True, right_index=True))\n",
    "\n",
    "y_train = np.log1p(y_train)\n",
    "y_val = np.log1p(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df: pd.DataFrame, data_origin: str, predictor: str = ''):\n",
    "    # Define datasets to be merged\n",
    "    age_groups_merge = group_ages(age, age_ranges)\n",
    "    spatial_merge = spatial_latest.drop(columns=['year'])\n",
    "    income_merge = income_latest.drop(columns=['year'])\n",
    "    households_merge = households_latest.drop(columns=['year'])\n",
    "    plaace_merge = plaace.drop_duplicates(subset='plaace_hierarchy_id')\n",
    "    bus_data_train_merge = gpd.read_parquet(f'derived_data/stores_bus_stops_lt_1km_{data_origin}').drop(columns=['geometry'])\n",
    "    stores_vicinity_merge = gpd.read_parquet(f'derived_data/stores_count_lt_1km_{data_origin}').drop(columns=['geometry'])\n",
    "\n",
    "    # Merge datasets\n",
    "    df = df.merge(age_groups_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(spatial_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(income_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(households_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(plaace_merge, how='left')\n",
    "    df = df.merge(bus_data_train_merge, on='store_id', how='left')\n",
    "    df = df.merge(stores_vicinity_merge, on='store_id', how='left')\n",
    "    df = add_city_centre_dist(df).drop(columns=['lon_center', 'lat_center'])\n",
    "\n",
    "    # Transformations and some post-merge cleaning\n",
    "    df.stores_count_lt_1km = np.log(df.stores_count_lt_1km)\n",
    "    df[age_groups_merge.columns] = df[age_groups_merge.columns].fillna(0)\n",
    "    \n",
    "    # Handle categories for different predictors\n",
    "    if predictor == 'xgb':\n",
    "        df = object_encoder(df)\n",
    "    elif predictor == 'cb':\n",
    "        df = nan_to_string(df)\n",
    "    elif predictor == 'lgb':\n",
    "        df = to_categorical(df)\n",
    "    else: \n",
    "        raise ValueError('Invalid predictor')\n",
    "\n",
    "    features = [\n",
    "        'store_name', \n",
    "        'mall_name', \n",
    "        'chain_name',\n",
    "        'address', \n",
    "        'lat', 'lon',\n",
    "        \n",
    "        *age_groups_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        *income_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        *households_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        'lv1_desc', 'lv2_desc', 'sales_channel_name', \n",
    "        *bus_data_train_merge.drop(columns=['store_id']).columns,\n",
    "        *stores_vicinity_merge.drop(columns=['store_id']).columns,\n",
    "        'dist_to_center'\n",
    "    ]\n",
    "\n",
    "    return df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features adapted to Catboost\n",
    "X_train_cb = generate_features(X_train, data_origin='train', predictor='cb')\n",
    "X_val_cb = generate_features(X_val, data_origin='train', predictor='cb')\n",
    "X_test_cb = generate_features(test, data_origin='test', predictor='cb')\n",
    "\n",
    "# Features adapted to LightGBM\n",
    "X_train_lgb = generate_features(X_train, data_origin='train', predictor='lgb')\n",
    "X_val_lgb = generate_features(X_val, data_origin='train', predictor='lgb')\n",
    "X_test_lgb = generate_features(test, data_origin='test', predictor='lgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing pools and parameter grid for Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cb_pools():\n",
    "    text_features = ['store_name', 'address', 'sales_channel_name'] \n",
    "    cat_features = ['mall_name', 'chain_name', 'lv1_desc', 'lv2_desc']\n",
    "\n",
    "    train_pool = cb.Pool(\n",
    "        X_train_cb,\n",
    "        y_train,\n",
    "        cat_features=cat_features,\n",
    "        text_features=text_features,\n",
    "        feature_names=list(X_train_cb)\n",
    "    )\n",
    "\n",
    "    valid_pool = cb.Pool(\n",
    "        X_val_cb,\n",
    "        y_val,\n",
    "        cat_features=cat_features,\n",
    "        text_features=text_features,\n",
    "        feature_names=list(X_train_cb)\n",
    "    )\n",
    "\n",
    "    return train_pool, valid_pool\n",
    "\n",
    "\n",
    "def get_cb_params(trial: optuna.Trial = None):\n",
    "    gpu_count = get_gpu_device_count()\n",
    "    non_tunable_cb_params = {\n",
    "        'objective': 'RMSE',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'task_type': 'GPU' if gpu_count else 'CPU', \n",
    "        'devices': f'0:{gpu_count}',\n",
    "        'random_seed': SEED\n",
    "    }\n",
    "\n",
    "    if trial is None:\n",
    "        return 'cb', non_tunable_cb_params\n",
    "    \n",
    "    tunable_params = {\n",
    "        'depth': trial.suggest_int('depth', 4, 9),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['Ordered', 'Plain']),\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 2, 6),\n",
    "        # 'iterations': trial.suggest_int('iterations', 1000, 2000),\n",
    "        # 'learning_rate': trial.suggest_categorical('learning_rate', 0.1, 0.5)\n",
    "    }\n",
    "\n",
    "    return 'cb', non_tunable_cb_params, tunable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing DMatrices and parameter grid for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lgb_dmatrices():\n",
    "    dtrain = lgb.Dataset(X_train_lgb, y_train, params={'verbose': -1}, free_raw_data=False)\n",
    "    dvalid = lgb.Dataset(X_val_lgb, y_val, params={'verbose': -1}, free_raw_data=False)\n",
    "    return dtrain, dvalid\n",
    "\n",
    "\n",
    "rmsle_scorer = make_scorer(lambda y, y_true: rmsle(y, y_true), greater_is_better=False)\n",
    "\n",
    "def get_lgb_params(trial: optuna.Trial = None):\n",
    "    non_tunable_lgb_params = {\n",
    "        'objective': 'rmse',\n",
    "        'verbose': -1,\n",
    "        'seed': 1\n",
    "    }\n",
    "\n",
    "    if trial is None:\n",
    "        return 'lgb', non_tunable_lgb_params\n",
    "\n",
    "    tunable_params = {\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'goss', 'dart']),\n",
    "    }\n",
    "\n",
    "    if tunable_params['boosting_type'] != 'goss':\n",
    "        tunable_params[\"bagging_fraction\"]: trial.suggest_float(\"bagging_fraction\", 0.4, 1.0)\n",
    "        tunable_params[\"bagging_freq\"]: trial.suggest_int(\"bagging_freq\", 1, 7)\n",
    "\n",
    "    return 'lgb', non_tunable_lgb_params, tunable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(trial: optuna.Trial, param_grid_fn: Callable) -> float:\n",
    "    model_name, non_tunable_params, tunable_params = param_grid_fn(trial)\n",
    "  \n",
    "    if model_name == 'cb':\n",
    "        if tunable_params['bootstrap_type'] == 'Bayesian': \n",
    "            tunable_params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n",
    "        elif tunable_params['bootstrap_type'] == 'Bernoulli':\n",
    "            tunable_params['subsample'] = trial.suggest_float('subsample', 0.1, 1, log=True)\n",
    "\n",
    "        cbr = cb.CatBoostRegressor(**non_tunable_params, **tunable_params) \n",
    "        train_pool, valid_pool = get_cb_pools()\n",
    "        cbr.fit(\n",
    "            train_pool,\n",
    "            eval_set=[(X_val_cb, y_val)],\n",
    "            verbose=0,\n",
    "            early_stopping_rounds=100,\n",
    "        )\n",
    "        y_pred = cbr.predict(X_val_cb)\n",
    "    \n",
    "    elif model_name == 'lgb':\n",
    "        dtrain_lgb, dvalid_lgb = get_lgb_dmatrices()\n",
    "        lgbr = lgb.train(\n",
    "            params={**non_tunable_params, **tunable_params},\n",
    "            train_set=dtrain_lgb,\n",
    "            valid_sets=dvalid_lgb,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "        y_pred = lgbr.predict(X_val_lgb)\n",
    "\n",
    "    score = rmsle(np.expm1(y_val), np.expm1(y_pred))\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_hyper_parameters(param_grid_fn: Callable, n_trials=100):\n",
    "    study = optuna.create_study(\n",
    "        study_name='hyperparam-tuning',\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), \n",
    "        direction='minimize'\n",
    "    )\n",
    "    objective_fn = lambda trial: objective(trial, param_grid_fn)\n",
    "    study.optimize(objective_fn, n_trials=n_trials, timeout=900) \n",
    "\n",
    "    print('Number of finished trials: {}'.format(len(study.trials)))\n",
    "    \n",
    "    trial = study.best_trial\n",
    "    print(f'Best trial ({trial.number}):')\n",
    "    print('Value:', trial.value)\n",
    "    print('Params:')\n",
    "    print(trial.params)\n",
    "\n",
    "    return param_grid_fn()[1], trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_tunable_cb_params, tuned_params = get_hyper_parameters(get_cb_params, n_trials=30)\n",
    "train_pool, valid_pool = get_cb_pools()\n",
    "cbm = cb.CatBoostRegressor(**non_tunable_cb_params, **tuned_params, iterations=1000) \n",
    "cbm.fit(train_pool, eval_set=valid_pool, verbose=50, plot=True, early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_tunable_lgb_params, tunable_lgb_params = get_hyper_parameters(get_lgb_params, n_trials=400)\n",
    "dtrain_lgb, dvalid_lgb = get_lgb_dmatrices()\n",
    "lgbm = lgb.train(\n",
    "    params={**non_tunable_lgb_params, **tunable_lgb_params},\n",
    "    train_set=dtrain_lgb,\n",
    "    valid_sets=dvalid_lgb,\n",
    "    verbose_eval=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain_lgb, dvalid_lgb = get_lgb_dmatrices()\n",
    "lgbm = lgb.train(\n",
    "    params={**get_lgb_params()[1], **{'lambda_l1': 0.026873437304227192, 'lambda_l2': 9.270495297846024, 'num_leaves': 7, 'feature_fraction': 0.8633788253368204, 'min_child_samples': 81, 'boosting_type': 'gbdt'}},\n",
    "    train_set=dtrain_lgb,\n",
    "    valid_sets=dvalid_lgb,\n",
    "    verbose_eval=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost validation prediction\n",
    "y_pred_val_cb = np.expm1(cbm.predict(X_val_cb))\n",
    "print('LightGBM validation score:', rmsle(np.expm1(y_val), y_pred_val_cb))\n",
    "\n",
    "# Catboost validation prediction\n",
    "y_val_pred_lgb = np.expm1(lgbm.predict(X_val_lgb))\n",
    "print('LightGBM validation score:', rmsle(np.expm1(y_val), y_val_pred_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_cb = np.expm1(cbm.predict(X_test_cb))\n",
    "y_pred_test_lgb = np.expm1(lgbm.predict(X_test_lgb))\n",
    "\n",
    "test_stack = np.array([y_pred_test_cb, y_pred_test_lgb])\n",
    "stack_test_avg = np.mean(test_stack, axis=0)\n",
    "\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['predicted'] = stack_test_avg\n",
    "submission.to_csv('submissions/cb_lgb_automl_stacked2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1beb4c19f9c9850d9088b15a4a1b3063555a573f9fb8d1ae667cbe7a8ada917e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
