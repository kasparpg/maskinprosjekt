{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import warnings\n",
    "import os.path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopy\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import shutil\n",
    "import geopandas as gpd\n",
    "import catboost as cb\n",
    "\n",
    "from xgboost import XGBClassifier, plot_importance, to_graphviz, plot_tree\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "from k_fold import random_k_fold\n",
    "from shapely import wkt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from utils import squared_log, rmsle_xgb, add_city_centre_dist, group_ages, to_categorical, nan_to_string, object_encoder\n",
    "from k_fold import random_k_fold, _rmsle\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "spatial = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "age = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "income = pd.read_csv('data/grunnkrets_income_households.csv')\n",
    "households = pd.read_csv('data/grunnkrets_households_num_persons.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "plaace = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "busstops = pd.read_csv('data/busstops_norway.csv')\n",
    "\n",
    "train = pd.read_csv('data/stores_train.csv')\n",
    "test = pd.read_csv('data/stores_test.csv')\n",
    "\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "model_name = \"modeling/0002.model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df: pd.DataFrame, predictor: str = ''):\n",
    "    features = ['store_id', 'year', 'store_name', 'mall_name', 'chain_name', 'address', 'lat', 'lon',\n",
    "                'plaace_hierarchy_id', 'grunnkrets_id']\n",
    "    _X = df[features]\n",
    "\n",
    "    _X['store_name'] = _X['store_name']\n",
    "    _X['address'] = _X['address']\n",
    "    _X['mall_name'] = _X['mall_name']\n",
    "    _X['chain_name'] = _X['chain_name']\n",
    "    _X['plaace_hierarchy_id'] = _X['plaace_hierarchy_id']\n",
    "    # _X['latlon'] = f'{_X.lat}{_X.lon}'\n",
    "    # _X['latlon'] = _X['latlon'].astype('category')\n",
    "\n",
    "    # Merge spatial data\n",
    "    _X = _X.merge(spatial.drop(columns=['year']).drop_duplicates(subset=['grunnkrets_id']), on='grunnkrets_id', how='left')\n",
    "    _X['grunnkrets_name'] = _X['grunnkrets_name']\n",
    "    _X['district_name'] = _X['district_name']\n",
    "    _X['municipality_name'] = _X['municipality_name']\n",
    "    _X = _X.drop(columns=['geometry'])\n",
    "\n",
    "    # Merge age data\n",
    "    age_ranges = [\n",
    "        (0, 19),\n",
    "        (20, 39),\n",
    "        (40, 59),\n",
    "        (60, 79),\n",
    "        (80, 90),\n",
    "    ]\n",
    "    grouped_ages = group_ages(age, age_ranges)\n",
    "    _X = _X.merge(grouped_ages, on='grunnkrets_id', how='left')\n",
    "\n",
    "    # Merge income data\n",
    "    _X = _X.merge(income.drop(columns=['year']).drop_duplicates(subset='grunnkrets_id'), how='left')\n",
    "\n",
    "    # Merge household data\n",
    "    _X = _X.merge(households.drop(columns=['year']).drop_duplicates(subset='grunnkrets_id'), how='left')\n",
    "\n",
    "    # Merge plaace data\n",
    "    _X = _X.merge(plaace.drop_duplicates(subset='plaace_hierarchy_id'), how='left')\n",
    "    _X['plaace_hierarchy_id'] = _X['plaace_hierarchy_id']\n",
    "    _X['sales_channel_name'] = _X['sales_channel_name']\n",
    "    _X = _X.drop(columns=['lv1', 'lv2', 'lv3', 'lv4'])\n",
    "    _X['lv1_desc'] = _X['lv1_desc']\n",
    "    _X['lv2_desc'] = _X['lv2_desc']\n",
    "    _X['lv3_desc'] = _X['lv3_desc']\n",
    "    _X['lv4_desc'] = _X['lv4_desc']\n",
    "    \n",
    "    _X = add_city_centre_dist(_X).drop(columns=['lon_center', 'lat_center'])\n",
    "\n",
    "    # Merge bus data\n",
    "    bus_data_train = gpd.read_parquet('derived_data/stores_bus_stops_lt_1km_train')\n",
    "    _X = _X.merge(bus_data_train.drop(columns=['geometry']), on='store_id', how='left')\n",
    "\n",
    "    _X = _X.drop(columns=['grunnkrets_id', 'plaace_hierarchy_id', 'year', 'store_id'])\n",
    "\n",
    "    if predictor == 'xgb':\n",
    "        # _X = to_categorical(_X)\n",
    "        _X = object_encoder(_X)\n",
    "    elif predictor == 'catboost':\n",
    "        print('hei')\n",
    "        _X = nan_to_string(_X)\n",
    "        print(_X.isna().sum())\n",
    "        \n",
    "    return _X\n",
    "\n",
    "\n",
    "label_name = 'revenue'\n",
    "X = train.drop(columns=[label_name])\n",
    "y = train[label_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(data):\n",
    "  df = data[['revenue', \n",
    "    # 'age_0_19', 'age_20_39', 'age_40_59', 'age_60_79', 'age_80_90', \n",
    "    # 'bus_stops_count', 'Mangler viktighetsniv√•', 'Standard holdeplass', 'Lokalt knutepunkt', 'Nasjonalt knutepunkt', 'Regionalt knutepunkt', 'Annen viktig holdeplass', \n",
    "    'dist_to_center', 'lat','lon'\n",
    "    ]]\n",
    "  df['knutepunkt'] = data[['Lokalt knutepunkt', 'Nasjonalt knutepunkt', 'Regionalt knutepunkt']].sum(axis=1)\n",
    "  # df.revenue = np.exp(df.revenue)\n",
    "  # df.bus_stops_count = np.sqrt(df.bus_stops_count)\n",
    "  df = df[df.dist_to_center < 70_000]\n",
    "  # df.dist_to_center = np.log(df.dist_to_center)\n",
    "  \n",
    "  plt.figure(figsize=(15, 15))\n",
    "  pairplot = sns.pairplot(df)\n",
    "  # heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\n",
    "\n",
    "\n",
    "# data_full =  pd.merge(X_train, y_train, left_index=True, right_index=True) \n",
    "# plot_corr(data_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_buffers(X_train, y_train, X_val, y_val):\n",
    "    # Clear buffers\n",
    "    folder = os.path.join(os.getcwd(), 'modeling')\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "            print(f'Deleted file: {file_path}')\n",
    "\n",
    "    train_buffer_path = 'modeling/train.buffer'\n",
    "    test_buffer_path = 'modeling/test.buffer'\n",
    "\n",
    "    dtrain = xgb.DMatrix(data=X_train, label=y_train, enable_categorical=True)\n",
    "    dtrain.save_binary(train_buffer_path)\n",
    "    print(f'--> {train_buffer_path} created and saved.')\n",
    "\n",
    "    dvalid = xgb.DMatrix(data=X_val, label=y_val, enable_categorical=True)\n",
    "    dvalid.save_binary(test_buffer_path)\n",
    "    print(f'--> {test_buffer_path} created and saved.')\n",
    "\n",
    "    return dtrain, dvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing and creating buffers...\n",
      "Deleted file: /home/koholm/Desktop/maskinprosjekt/modeling/train.buffer\n",
      "Deleted file: /home/koholm/Desktop/maskinprosjekt/modeling/test.buffer\n",
      "Deleted file: /home/koholm/Desktop/maskinprosjekt/modeling/0002.model\n",
      "--> modeling/train.buffer created and saved.\n",
      "--> modeling/test.buffer created and saved.\n",
      "<xgboost.core.DMatrix object at 0x7f68af0eb9d0> <xgboost.core.DMatrix object at 0x7f686ca19520>\n",
      "Attempting to initialize parameters for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koholm/Desktop/maskinprosjekt/k_fold.py:21: RuntimeWarning: invalid value encountered in log1p\n",
      "  elements = np.power(np.log1p(y_pred) - np.log1p(y_true), 2)\n",
      "/home/koholm/Desktop/maskinprosjekt/k_fold.py:21: RuntimeWarning: invalid value encountered in log1p\n",
      "  elements = np.power(np.log1p(y_pred) - np.log1p(y_true), 2)\n",
      "/home/koholm/Desktop/maskinprosjekt/k_fold.py:21: RuntimeWarning: invalid value encountered in log1p\n",
      "  elements = np.power(np.log1p(y_pred) - np.log1p(y_true), 2)\n",
      "/home/koholm/Desktop/maskinprosjekt/k_fold.py:21: RuntimeWarning: invalid value encountered in log1p\n",
      "  elements = np.power(np.log1p(y_pred) - np.log1p(y_true), 2)\n",
      "/home/koholm/Desktop/maskinprosjekt/k_fold.py:21: RuntimeWarning: invalid value encountered in log1p\n",
      "  elements = np.power(np.log1p(y_pred) - np.log1p(y_true), 2)\n",
      "/home/koholm/Desktop/maskinprosjekt/k_fold.py:21: RuntimeWarning: invalid value encountered in log1p\n",
      "  elements = np.power(np.log1p(y_pred) - np.log1p(y_true), 2)\n",
      "/home/koholm/Desktop/maskinprosjekt/k_fold.py:21: RuntimeWarning: invalid value encountered in log1p\n",
      "  elements = np.power(np.log1p(y_pred) - np.log1p(y_true), 2)\n",
      "/home/koholm/Desktop/maskinprosjekt/k_fold.py:21: RuntimeWarning: invalid value encountered in log1p\n",
      "  elements = np.power(np.log1p(y_pred) - np.log1p(y_true), 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 447.63 seconds for 100 candidates parameter settings.\n",
      "{'colsample_bytree': 0.4959821240615436, 'eval_metric': <function _rmsle at 0x7f686daea9d0>, 'gamma': 0.4638144504203367, 'learning_rate': 0.06423808417748789, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 229, 'objective': 'reg:squaredlogerror', 'subsample': 0.9629227852893962}\n",
      "--> parameters for training initialized.\n",
      "0.6718127793885331\n",
      "\n",
      "Attempting to start prediction...\n",
      "--> Prediction finished.\n",
      "\n",
      "Attempting to save prediction...\n",
      "--> prediction saved with features as name in submission folder.\n",
      "[4.1228914 5.9252825 5.725103  ... 4.818073  2.1065047 2.0544095]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def train_xgb_model(X_train, y_train, X_val, y_val):\n",
    "    print('Clearing and creating buffers...')\n",
    "    dtrain, dvalid = clear_buffers(X_train, y_train, X_val, y_val)\n",
    "    print(dtrain, dvalid)\n",
    "\n",
    "    print(\"Attempting to initialize parameters for training...\")\n",
    "\n",
    "    params = {'colsample_bytree': 0.7717138210314867, 'learning_rate': 0.047506668950627134, 'max_depth': 8, 'min_child_weight': 3, 'n_estimators': 223, 'subsample': 0.9929036803032936}\n",
    "    \n",
    "    rand_src_model = random_k_fold(X_train, y_train, verbose=0, n_iter=100)\n",
    "    params = rand_src_model.best_params_\n",
    "    print(params)\n",
    "\n",
    "    # params = {'colsample_bytree': 0.8601277878899238, 'eval_metric': _rmsle, 'gamma': 0.12760202929262826, 'learning_rate': 0.07356461924449906, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 306, 'objective': 'reg:squaredlogerror', 'subsample': 0.8993341396761092}\n",
    "\n",
    "    params['disable_default_eval_metric'] = True\n",
    "    print(\"--> parameters for training initialized.\")\n",
    "\n",
    "    y_pred = model.predict(dvalid)\n",
    "    print(mean_squared_log_error(y_val, y_pred))\n",
    "\n",
    "    X_test = generate_features(test, predictor='xgb')\n",
    "    dtest = xgb.DMatrix(data=X_test, enable_categorical=True)\n",
    "\n",
    "    print(\"\\nAttempting to start prediction...\")\n",
    "    y_pred = model.predict(dtest, ntree_limit=model.best_iteration)\n",
    "    print(\"--> Prediction finished.\")\n",
    "\n",
    "    print(\"\\nAttempting to save prediction...\")\n",
    "    submission['predicted'] = np.array(y_pred)\n",
    "    submission.to_csv('submissions/submission.csv', index=False)\n",
    "    print(\"--> prediction saved with features as name in submission folder.\")\n",
    "\n",
    "    # num_round = 999\n",
    "    # watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "    # print(\"Attempting to start training...\")\n",
    "    # model = xgb.train(\n",
    "    #     params=params, \n",
    "    #     dtrain=dtrain, \n",
    "    #     num_boost_round=num_round, \n",
    "    #     obj=squared_log,\n",
    "    #     custom_metric=rmsle_xgb,\n",
    "    #     evals=watchlist, \n",
    "    #     early_stopping_rounds=10, \n",
    "    #     verbose_eval=20)\n",
    "    # print(\"--> model trained.\")\n",
    "    # print('Best score:', model.best_score)\n",
    "\n",
    "    # print(\"Attempting to save model...\")\n",
    "    # model.save_model(model_name)\n",
    "    # print(\"--> model saved.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=.8)\n",
    "X_train, X_val = generate_features(X_train, predictor='xgb'), generate_features(X_val, predictor='xgb')\n",
    "\n",
    "model = train_xgb_model(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to start prediction...\n",
      "--> Prediction finished.\n",
      "\n",
      "Attempting to save prediction...\n",
      "--> prediction saved with features as name in submission folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def xgb_prediction(X_test, model):\n",
    "    dtest = xgb.DMatrix(data=X_test, enable_categorical=True)\n",
    "\n",
    "    print(\"\\nAttempting to start prediction...\")\n",
    "    y_pred = model.predict(dtest, ntree_limit=model.best_iteration)\n",
    "    print(\"--> Prediction finished.\")\n",
    "\n",
    "    print(\"\\nAttempting to save prediction...\")\n",
    "    submission['predicted'] = np.array(y_pred)\n",
    "    submission.to_csv('submissions/submission.csv', index=False)\n",
    "    print(\"--> prediction saved with features as name in submission folder.\")\n",
    "\n",
    "\n",
    "X_test = generate_features(test, predictor='xgb')\n",
    "xgb_prediction(X_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_importance(model)\n",
    "# xgb.to_graphviz(model, num_trees=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features for Catboost predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=.8)\n",
    "# X_train, X_val = generate_features(X_train, predictor='catboost'), generate_features(X_val, predictor='catboost')\n",
    "# X_test = generate_features(test, predictor='catboost')\n",
    "\n",
    "# cat_features = list(X_train.select_dtypes(include=[object]).columns)\n",
    "\n",
    "# train_pool = cb.Pool(X_train, y_train, cat_features=cat_features)\n",
    "# test_pool = cb.Pool(X_test, cat_features=cat_features)\n",
    "\n",
    "# # X_train[X_train.columns[X_train.isna().any()].tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'depth': randint(2, 20),\n",
    "#     'learning_rate': uniform(0.01, 0.4),\n",
    "#     'iterations': randint(10, 1000)\n",
    "# }\n",
    "\n",
    "# model = cb.CatBoostRegressor(loss_function='RMSE')\n",
    "\n",
    "# model.randomized_search(train_pool, param_distributions=params, cv=5)\n",
    "\n",
    "# pred = model.predict(X_val)\n",
    "# rmse = (np.sqrt(mean_squared_log_error(y_val, pred)))\n",
    "# print('Testing performance')\n",
    "# print('RMSE: {:.2f}'.format(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1beb4c19f9c9850d9088b15a4a1b3063555a573f9fb8d1ae667cbe7a8ada917e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
