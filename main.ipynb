{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install and import libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asttokens==2.1.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: attrs==22.1.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (22.1.0)\n",
      "Requirement already satisfied: autopep8==2.0.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: catboost==1.1.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.1.1)\n",
      "Requirement already satisfied: certifi==2022.9.24 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (2022.9.24)\n",
      "Requirement already satisfied: click==8.1.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (8.1.3)\n",
      "Requirement already satisfied: click-plugins==1.1.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (1.1.1)\n",
      "Requirement already satisfied: cligj==0.7.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (0.7.2)\n",
      "Requirement already satisfied: colorama==0.4.6 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (0.4.6)\n",
      "Requirement already satisfied: contourpy==1.0.6 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (1.0.6)\n",
      "Requirement already satisfied: cycler==0.11.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (0.11.0)\n",
      "Requirement already satisfied: Cython==0.29.28 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (0.29.28)\n",
      "Requirement already satisfied: debugpy==1.6.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (1.6.3)\n",
      "Requirement already satisfied: decorator==5.1.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (5.1.1)\n",
      "Requirement already satisfied: entrypoints==0.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (0.4)\n",
      "Requirement already satisfied: executing==1.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (1.2.0)\n",
      "Requirement already satisfied: Fiona==1.8.22 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 18)) (1.8.22)\n",
      "Requirement already satisfied: fonttools==4.38.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 19)) (4.38.0)\n",
      "Requirement already satisfied: gensim==4.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 20)) (4.2.0)\n",
      "Requirement already satisfied: geographiclib==1.52 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 21)) (1.52)\n",
      "Requirement already satisfied: geopandas==0.12.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 22)) (0.12.1)\n",
      "Requirement already satisfied: geopy==2.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 23)) (2.2.0)\n",
      "Requirement already satisfied: graphviz==0.20.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 24)) (0.20.1)\n",
      "Requirement already satisfied: ipykernel==6.17.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 25)) (6.17.0)\n",
      "Requirement already satisfied: ipython==8.6.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 26)) (8.6.0)\n",
      "Requirement already satisfied: jedi==0.18.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 27)) (0.18.1)\n",
      "Requirement already satisfied: joblib==1.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 28)) (1.2.0)\n",
      "Requirement already satisfied: jupyter_client==7.4.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 29)) (7.4.4)\n",
      "Requirement already satisfied: jupyter_core==4.11.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 30)) (4.11.2)\n",
      "Requirement already satisfied: kiwisolver==1.4.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 31)) (1.4.4)\n",
      "Requirement already satisfied: matplotlib==3.6.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 32)) (3.6.0)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 33)) (0.1.6)\n",
      "Requirement already satisfied: munch==2.5.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 34)) (2.5.0)\n",
      "Requirement already satisfied: nest-asyncio==1.5.6 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 35)) (1.5.6)\n",
      "Requirement already satisfied: numpy==1.23.4 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 36)) (1.23.4)\n",
      "Requirement already satisfied: packaging==21.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 37)) (21.3)\n",
      "Requirement already satisfied: pandas==1.5.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 38)) (1.5.1)\n",
      "Requirement already satisfied: parso==0.8.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 39)) (0.8.3)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 40)) (0.7.5)\n",
      "Requirement already satisfied: Pillow==9.3.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 41)) (9.3.0)\n",
      "Requirement already satisfied: plotly==5.11.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 42)) (5.11.0)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.31 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 43)) (3.0.31)\n",
      "Requirement already satisfied: psutil==5.9.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 44)) (5.9.3)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 45)) (0.2.2)\n",
      "Requirement already satisfied: pyarrow==10.0.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 46)) (10.0.0)\n",
      "Requirement already satisfied: pycodestyle==2.9.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 47)) (2.9.1)\n",
      "Requirement already satisfied: pygeos==0.13 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 48)) (0.13)\n",
      "Requirement already satisfied: Pygments==2.13.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 49)) (2.13.0)\n",
      "Requirement already satisfied: pyparsing==3.0.9 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 50)) (3.0.9)\n",
      "Requirement already satisfied: pyproj==3.4.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 51)) (3.4.0)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 52)) (2.8.2)\n",
      "Requirement already satisfied: pytz==2022.6 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 53)) (2022.6)\n",
      "Requirement already satisfied: pyzmq==24.0.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 54)) (24.0.1)\n",
      "Requirement already satisfied: scikit-learn==1.1.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 55)) (1.1.3)\n",
      "Requirement already satisfied: scipy==1.9.3 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 56)) (1.9.3)\n",
      "Requirement already satisfied: seaborn==0.12.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 57)) (0.12.1)\n",
      "Requirement already satisfied: Shapely==1.8.5.post1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 58)) (1.8.5.post1)\n",
      "Requirement already satisfied: six==1.16.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 59)) (1.16.0)\n",
      "Requirement already satisfied: sklearn==0.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 60)) (0.0)\n",
      "Requirement already satisfied: smart-open==6.2.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 61)) (6.2.0)\n",
      "Requirement already satisfied: stack-data==0.6.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 62)) (0.6.0)\n",
      "Requirement already satisfied: tenacity==8.1.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 63)) (8.1.0)\n",
      "Requirement already satisfied: threadpoolctl==3.1.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 64)) (3.1.0)\n",
      "Requirement already satisfied: tomli==2.0.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 65)) (2.0.1)\n",
      "Requirement already satisfied: tornado==6.2 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 66)) (6.2)\n",
      "Requirement already satisfied: tqdm==4.64.1 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 67)) (4.64.1)\n",
      "Requirement already satisfied: traitlets==5.5.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 68)) (5.5.0)\n",
      "Requirement already satisfied: wcwidth==0.2.5 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 69)) (0.2.5)\n",
      "Requirement already satisfied: xgboost==1.7.0 in ./venv/lib/python3.8/site-packages (from -r requirements.txt (line 70)) (1.7.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from Fiona==1.8.22->-r requirements.txt (line 18)) (44.0.0)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in ./venv/lib/python3.8/site-packages (from ipython==8.6.0->-r requirements.txt (line 26)) (4.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.8/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython==8.6.0->-r requirements.txt (line 26)) (0.7.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import warnings\n",
    "import os.path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopy\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import shutil\n",
    "import geopandas as gpd\n",
    "import catboost as cb\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "from xgboost import XGBRegressor, plot_importance, to_graphviz, plot_tree\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "from k_fold import random_k_fold\n",
    "from shapely import wkt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from utils import squared_log, rmsle_xgb, add_city_centre_dist, group_ages, to_categorical, nan_to_string, object_encoder, only_2016_data\n",
    "from k_fold import random_k_fold, xgb_cross_validation\n",
    "from objectives_and_metrics import rmsle, RmsleMetric, RmsleObjective, LogTargetsRmsleMetric, RmseObjective\n",
    "from scipy.stats import uniform, randint\n",
    "from typing import Callable, Dict\n",
    "from catboost.utils import get_gpu_device_count\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "spatial = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "age = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "income = pd.read_csv('data/grunnkrets_income_households.csv').set_index(['grunnkrets_id', 'year']).add_prefix('income_').reset_index()\n",
    "households = pd.read_csv('data/grunnkrets_households_num_persons.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "plaace = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "busstops = pd.read_csv('data/busstops_norway.csv')\n",
    "\n",
    "train = pd.read_csv('data/stores_train.csv')\n",
    "test = pd.read_csv('data/stores_test.csv') \n",
    "\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "model_name = \"modeling/0002.model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.x Data cleaning\n",
    "\n",
    "The train and test data only contains data from 2016, so for the other datasets with an age column\n",
    "we only use the values from 2016, where possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_ranges = [\n",
    "    (0, 19),\n",
    "    (20, 39),\n",
    "    (40, 59),\n",
    "    (60, 79),\n",
    "    (80, 90),\n",
    "]\n",
    "\n",
    "spatial_2016 = only_2016_data(spatial)\n",
    "income_2016 = only_2016_data(income)\n",
    "households_2016 = only_2016_data(households)\n",
    "\n",
    "train_spatial = train.merge(spatial_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "muni_avg_revenue = train_spatial.groupby(by='municipality_name', as_index=False)['revenue'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Futhermore, we noticed that a number of rows in the train and test datasets didn't have  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned out 805 out of 12859 rows.\n",
      "Length of data frame: 9643\n",
      "Length after removing extreme values and zero revenue retail stores: 9429\n"
     ]
    }
   ],
   "source": [
    "def clean(df: pd.DataFrame, min_val=0, max_val=100):\n",
    "    print('Length of data frame:', len(df))\n",
    "    df = df[(df.revenue > min_val) & (df.revenue < max_val)]\n",
    "    print('Length after removing extreme values and zero revenue retail stores:',  len(df))\n",
    "    # plt.hist(np.log1p(train.revenue), 30)\n",
    "    # plt.show()\n",
    "    return df.drop(columns=['revenue']), df.revenue\n",
    "\n",
    "\n",
    "def clean_out_nan_heavy_rows(df: pd.DataFrame):\n",
    "    \"\"\"Cleans out rows that have no match in the age, spatial, income or household datasets.\"\"\"\n",
    "\n",
    "    df2 = df.merge(group_ages(age, age_ranges), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(spatial_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(income_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "    df2 = df2.merge(households_2016.drop(columns=['year']), on='grunnkrets_id', how='left')\n",
    "\n",
    "    df_cleaned = df[\n",
    "        ~(df2.age_0_19.isna() | df2.couple_children_0_to_5_years.isna() | df2.grunnkrets_name.isna() | df2.income_all_households.isna())  \n",
    "    ]\n",
    "\n",
    "    print(f'Cleaned out {len(df) - len(df_cleaned)} out of {len(df)} rows.')\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "train = clean_out_nan_heavy_rows(train)\n",
    "label_name = 'revenue'\n",
    "X = train.drop(columns=[label_name])\n",
    "y = train[label_name]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=.8) # , random_state=SEED\n",
    "X_train, y_train = clean(pd.merge(X_train, y_train, left_index=True, right_index=True))\n",
    "\n",
    "y_train = np.log1p(y_train)\n",
    "y_val = np.log1p(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df: pd.DataFrame, data_origin: str, predictor: str = ''):\n",
    "    # Define datasets to be merged\n",
    "    age_groups_merge = group_ages(age, age_ranges)\n",
    "    spatial_merge = spatial_2016.drop(columns=['year'])\n",
    "    income_merge = income_2016.drop(columns=['year'])\n",
    "    households_merge = households_2016.drop(columns=['year'])\n",
    "    plaace_merge = plaace.drop_duplicates(subset='plaace_hierarchy_id')\n",
    "    bus_data_train_merge = gpd.read_parquet(f'derived_data/stores_bus_stops_lt_1km_{data_origin}').drop(columns=['geometry'])\n",
    "    stores_vicinity_merge = gpd.read_parquet(f'derived_data/stores_count_lt_1km_{data_origin}').drop(columns=['geometry'])\n",
    "\n",
    "    # Merge datasets\n",
    "    df = df.merge(age_groups_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(spatial_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(muni_avg_revenue, on='municipality_name', how='left')\n",
    "    df = df.merge(income_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(households_merge, on='grunnkrets_id', how='left')\n",
    "    df = df.merge(plaace_merge, how='left')\n",
    "    df = df.merge(bus_data_train_merge, on='store_id', how='left')\n",
    "    df = df.merge(stores_vicinity_merge, on='store_id', how='left')\n",
    "    df = add_city_centre_dist(df).drop(columns=['lon_center', 'lat_center'])\n",
    "\n",
    "    # #Impute NaNs from spatial\n",
    "    # if sum(df[\"grunnkrets_name\"].isnull()):\n",
    "    #     df[\"grunnkrets_name\"].fillna(\"Ukjent grunnkrets\")\n",
    "\n",
    "    # Transformations\n",
    "    df.stores_count_lt_1km = df.stores_count_lt_1km.apply(np.log)\n",
    "\n",
    "    # Handle categories for different predictors\n",
    "    if predictor == 'xgb':\n",
    "        # df = to_categorical(df)\n",
    "        df = object_encoder(df)\n",
    "    elif predictor == 'catboost':\n",
    "        df = nan_to_string(df)\n",
    "    elif predictor == 'lgbm':\n",
    "        df = to_categorical(df)\n",
    "    else: \n",
    "        raise ValueError('Invalid predictor')\n",
    "\n",
    "    features = [\n",
    "        'store_name', \n",
    "        'mall_name', \n",
    "        'chain_name',\n",
    "        'address', \n",
    "        'lat', 'lon',\n",
    "        \n",
    "        *age_groups_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        *income_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        *households_merge.drop(columns=['grunnkrets_id']).columns,\n",
    "        'lv1_desc', 'lv2_desc', 'sales_channel_name',\n",
    "        *bus_data_train_merge.drop(columns=['store_id']).columns,\n",
    "        *stores_vicinity_merge.drop(columns=['store_id']).columns,\n",
    "        'dist_to_center'\n",
    "    ]\n",
    "\n",
    "    return df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features adapted to Catboost\n",
    "X_train_cb = generate_features(X_train, data_origin='train', predictor='catboost')\n",
    "X_val_cb = generate_features(X_val, data_origin='train', predictor='catboost')\n",
    "X_test_cb = generate_features(test, data_origin='test', predictor='catboost')\n",
    "\n",
    "# Features adapted to LightGBM\n",
    "X_train_lgb = generate_features(X_train, data_origin='train', predictor='lgbm')\n",
    "X_val_lgb = generate_features(X_val, data_origin='train', predictor='lgbm')\n",
    "X_test_lgb = generate_features(test, data_origin='test', predictor='lgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(data):\n",
    "  df = data[['revenue', \n",
    "    # 'age_0_19', 'age_20_39', 'age_40_59', 'age_60_79', 'age_80_90', \n",
    "    # 'bus_stops_count', 'Mangler viktighetsnivå', 'Standard holdeplass', 'Lokalt knutepunkt', 'Nasjonalt knutepunkt', 'Regionalt knutepunkt', 'Annen viktig holdeplass', \n",
    "    'dist_to_center', 'lat','lon'\n",
    "    ]]\n",
    "  df['knutepunkt'] = data[['Lokalt knutepunkt', 'Nasjonalt knutepunkt', 'Regionalt knutepunkt']].sum(axis=1)\n",
    "  # df.revenue = np.exp(df.revenue)\n",
    "  # df.bus_stops_count = np.sqrt(df.bus_stops_count)\n",
    "  df = df[df.dist_to_center < 70_000]\n",
    "  # df.dist_to_center = np.log(df.dist_to_center)\n",
    "  \n",
    "  plt.figure(figsize=(15, 15))\n",
    "  pairplot = sns.pairplot(df)\n",
    "  # heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\n",
    "\n",
    "\n",
    "# data_full =  pd.merge(X_train, y_train, left_index=True, right_index=True) \n",
    "# plot_corr(data_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_buffers(X_train, y_train, X_val, y_val):\n",
    "    # Clear buffers\n",
    "    folder = os.path.join(os.getcwd(), 'modeling')\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "            print(f'Deleted file: {file_path}')\n",
    "\n",
    "    train_buffer_path = 'modeling/train.buffer'\n",
    "    test_buffer_path = 'modeling/test.buffer'\n",
    "\n",
    "    dtrain = xgb.DMatrix(data=X_train, label=y_train, enable_categorical=True)\n",
    "    dtrain.save_binary(train_buffer_path)\n",
    "    print(f'--> {train_buffer_path} created and saved.')\n",
    "\n",
    "    dvalid = xgb.DMatrix(data=X_val, label=y_val, enable_categorical=True)\n",
    "    dvalid.save_binary(test_buffer_path)\n",
    "    print(f'--> {test_buffer_path} created and saved.')\n",
    "\n",
    "    return dtrain, dvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.best_score_)\n",
    "# y_pred_train = model.predict(X_train)\n",
    "# y_pred_val = model.predict(X_val)\n",
    "# print(rmsle(y_train, y_pred_train))\n",
    "# print(rmsle(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_model(X_train, y_train, X_val, y_val):\n",
    "    params = {'colsample_bytree': 0.7717138210314867, 'learning_rate': 0.047506668950627134, 'max_depth': 8, 'min_child_weight': 3, 'n_estimators': 223, 'subsample': 0.9929036803032936}\n",
    "    print('Clearing and creating buffers...')\n",
    "    dtrain, dvalid = clear_buffers(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    rand_search_model = random_k_fold(X_train, y_train, verbose=1, n_iter=100)\n",
    "    model = rand_search_model\n",
    "    params = model.best_params_\n",
    "    print(rand_search_model.best_score_, params)\n",
    "    \n",
    "    # params = {'colsample_bytree': 0.8601277878899238, 'eval_metric': 'rmsle', 'gamma': 0.12760202929262826, 'learning_rate': 0.07356461924449906, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 306, 'objective': 'reg:squaredlogerror', 'subsample': 0.8993341396761092}\n",
    "    \n",
    "    params['disable_default_eval_metric'] = True\n",
    "    # model = XGBRegressor()\n",
    "    # model.set_params(**params)\n",
    "    # model.fit(X_train, y_train)\n",
    "    # y_pred_train = model.predict(X_train)\n",
    "    # y_pred_val = model.predict(X_val)\n",
    "    # print(rmsle(y_train, y_pred_train))\n",
    "    # print(rmsle(y_val, y_pred_val))\n",
    "\n",
    "    # num_round = 999\n",
    "    # watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "    # print(\"Attempting to start training...\")\n",
    "    # model = xgb.train(\n",
    "    #     params=params, \n",
    "    #     dtrain=dtrain, \n",
    "    #     num_boost_round=num_round, \n",
    "    #     evals=watchlist, \n",
    "    #     early_stopping_rounds=10, \n",
    "    #     verbose_eval=20)\n",
    "    # print(\"--> model trained.\")\n",
    "    # print('Best score:', model.best_score)\n",
    "\n",
    "    # print(\"Attempting to save model...\")\n",
    "    # model.save_model(model_name)\n",
    "    # print(\"--> model saved.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=.8)\n",
    "# X_train, X_val = generate_features(X_train, predictor='xgb'), generate_features(X_val, predictor='xgb')\n",
    "\n",
    "# model = train_xgb_model(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_train = model.predict(X_train)\n",
    "# y_pred_val = model.predict(X_val)\n",
    "# print(rmsle(y_train, y_pred_train))\n",
    "# print(rmsle(y_val, y_pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_prediction(X_test, model):\n",
    "    dtest = xgb.DMatrix(data=X_test, enable_categorical=True)\n",
    "\n",
    "    print(\"\\nAttempting to start prediction...\")\n",
    "    y_pred = model.predict(dtest, ntree_limit=model.best_iteration)\n",
    "    print(\"--> Prediction finished.\")\n",
    "\n",
    "    print(\"\\nAttempting to save prediction...\")\n",
    "    submission['predicted'] = np.array(y_pred)\n",
    "    submission.to_csv('submissions/submission.csv', index=False)\n",
    "    print(\"--> prediction saved with features as name in submission folder.\")\n",
    "\n",
    "\n",
    "# X_test = generate_features(test, predictor='xgb')\n",
    "# xgb_prediction(X_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_model = model.best_estimator_ if model.best_estimator_ is not None else model\n",
    "# xgb_model = model\n",
    "# plot_importance(xgb_model)\n",
    "# xgb.to_graphviz(xgb_model, num_trees=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyper parameter tuning\n",
    "\n",
    "### Preparing pools and parameter grid for Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cb_pools():\n",
    "    # auxillary_columns = ['address']\n",
    "    text_features = ['store_name', 'address', 'sales_channel_name'] \n",
    "    cat_features = ['mall_name', 'chain_name', 'lv1_desc', 'lv2_desc']\n",
    "\n",
    "    train_pool = cb.Pool(\n",
    "        X_train_cb,\n",
    "        y_train,\n",
    "        cat_features=cat_features,\n",
    "        text_features=text_features,\n",
    "        feature_names=list(X_train_cb)\n",
    "    )\n",
    "\n",
    "    valid_pool = cb.Pool(\n",
    "        X_val_cb,\n",
    "        y_val,\n",
    "        cat_features=cat_features,\n",
    "        text_features=text_features,\n",
    "        feature_names=list(X_train_cb)\n",
    "    )\n",
    "\n",
    "    return train_pool, valid_pool\n",
    "\n",
    "\n",
    "def get_cb_params(trial: optuna.Trial = None):\n",
    "    gpu_count = get_gpu_device_count()\n",
    "    non_tunable_cb_params = {\n",
    "        'objective': 'RMSE',\n",
    "        'eval_metric': 'RMSE',\n",
    "        'task_type': 'GPU' if gpu_count else 'CPU', \n",
    "        'devices': f'0:{gpu_count}',\n",
    "        'random_seed': SEED\n",
    "    }\n",
    "\n",
    "    if trial is None:\n",
    "        return 'cb', non_tunable_cb_params\n",
    "    \n",
    "    tunable_params = {\n",
    "        'depth': trial.suggest_int('depth', 4, 9),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['Ordered', 'Plain']),\n",
    "        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n",
    "        # 'iterations': trial.suggest_int('iterations', 1000, 2000),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 2, 6),\n",
    "        # 'learning_rate': trial.suggest_categorical('learning_rate', 0.1, 0.5)\n",
    "    }\n",
    "\n",
    "    return 'cb', non_tunable_cb_params, tunable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing DMatrices and parameter grid for LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lgb_dmatrices():\n",
    "    dtrain = lgb.Dataset(X_train_lgb, y_train, params={'verbose': -1}, free_raw_data=False)\n",
    "    dvalid = lgb.Dataset(X_val_lgb, y_val, params={'verbose': -1}, free_raw_data=False)\n",
    "    return dtrain, dvalid\n",
    "\n",
    "\n",
    "def get_lgb_params(trial: optuna.Trial = None):\n",
    "    non_tunable_lgb_params = {\n",
    "        'objective': 'rmse',\n",
    "        'verbose': -1,\n",
    "        'seed': SEED\n",
    "    }\n",
    "\n",
    "    if trial is None:\n",
    "        return 'lgb', non_tunable_lgb_params\n",
    "\n",
    "    tunable_params = {\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'goss', 'dart']),\n",
    "    }\n",
    "\n",
    "    if tunable_params['boosting_type'] != 'goss':\n",
    "        tunable_params[\"bagging_fraction\"]: trial.suggest_float(\"bagging_fraction\", 0.4, 1.0)\n",
    "        tunable_params[\"bagging_freq\"]: trial.suggest_int(\"bagging_freq\", 1, 7)\n",
    "\n",
    "    return 'lgb', non_tunable_lgb_params, tunable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial, param_grid_fn: Callable) -> float:\n",
    "    model_name, non_tunable_params, tunable_params = param_grid_fn(trial)\n",
    "    \n",
    "    if model_name == 'cb':\n",
    "        if tunable_params['bootstrap_type'] == 'Bayesian': \n",
    "            tunable_params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n",
    "        elif tunable_params['bootstrap_type'] == 'Bernoulli':\n",
    "            tunable_params['subsample'] = trial.suggest_float('subsample', 0.1, 1, log=True)\n",
    "\n",
    "        cbr = cb.CatBoostRegressor(**non_tunable_params, **tunable_params) \n",
    "        train_pool, valid_pool = get_cb_pools()\n",
    "        cbr.fit(\n",
    "            train_pool,\n",
    "            eval_set=[(X_val_cb, y_val)],\n",
    "            verbose=0,\n",
    "            early_stopping_rounds=100,\n",
    "        )\n",
    "        y_pred = cbr.predict(X_val_cb)\n",
    "    \n",
    "    elif model_name == 'lgb':\n",
    "        dtrain_lgb, dvalid_lgb = get_lgb_dmatrices()\n",
    "        gbm = lgb.train(\n",
    "            params={**non_tunable_params, **tunable_params},\n",
    "            train_set=dtrain_lgb,\n",
    "            valid_sets=dvalid_lgb,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "        y_pred = gbm.predict(X_val_lgb)\n",
    "\n",
    "    score = rmsle(np.expm1(y_val), np.expm1(y_pred))\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_hyper_parameters(param_grid_fn: Callable, n_trials=100):\n",
    "    study = optuna.create_study(\n",
    "        study_name='hyperparam-tuning',\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), \n",
    "        direction='minimize'\n",
    "    )\n",
    "    objective_fn = lambda trial: objective(trial, param_grid_fn)\n",
    "    study.optimize(objective_fn, n_trials=n_trials, timeout=900) \n",
    "\n",
    "    print('Number of finished trials: {}'.format(len(study.trials)))\n",
    "    \n",
    "    trial = study.best_trial\n",
    "    print(f'Best trial ({trial.number}):')\n",
    "    print('Value:', trial.value)\n",
    "    print('Params:')\n",
    "    print(trial.params)\n",
    "\n",
    "    return param_grid_fn()[1], trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-10 20:34:55,000]\u001b[0m A new study created in memory with name: hyperparam-tuning\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['store_name', 'mall_name', 'chain_name', 'address', 'lat', 'lon', 'age_0_19', 'age_20_39', 'age_40_59', 'age_60_79', 'age_80_90', 'income_all_households', 'income_singles', 'income_couple_without_children', 'income_couple_with_children', 'income_other_households', 'income_single_parent_with_children', 'couple_children_0_to_5_years', 'couple_children_18_or_above', 'couple_children_6_to_17_years', 'couple_without_children', 'single_parent_children_0_to_5_years', 'single_parent_children_18_or_above', 'single_parent_children_6_to_17_years', 'singles', 'lv1_desc', 'lv2_desc', 'sales_channel_name', 'bus_stops_count', 'Mangler viktighetsnivå', 'Standard holdeplass', 'Lokalt knutepunkt', 'Nasjonalt knutepunkt', 'Regionalt knutepunkt', 'Annen viktig holdeplass', 'stores_count_lt_1km', 'dist_to_center']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-10 20:35:03,703]\u001b[0m Trial 0 finished with value: 0.7304170041341143 and parameters: {'depth': 8, 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli', 'l2_leaf_reg': 2.308509101283546, 'subsample': 0.15954430765120692}. Best is trial 0 with value: 0.7304170041341143.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['store_name', 'mall_name', 'chain_name', 'address', 'lat', 'lon', 'age_0_19', 'age_20_39', 'age_40_59', 'age_60_79', 'age_80_90', 'income_all_households', 'income_singles', 'income_couple_without_children', 'income_couple_with_children', 'income_other_households', 'income_single_parent_with_children', 'couple_children_0_to_5_years', 'couple_children_18_or_above', 'couple_children_6_to_17_years', 'couple_without_children', 'single_parent_children_0_to_5_years', 'single_parent_children_18_or_above', 'single_parent_children_6_to_17_years', 'singles', 'lv1_desc', 'lv2_desc', 'sales_channel_name', 'bus_stops_count', 'Mangler viktighetsnivå', 'Standard holdeplass', 'Lokalt knutepunkt', 'Nasjonalt knutepunkt', 'Regionalt knutepunkt', 'Annen viktig holdeplass', 'stores_count_lt_1km', 'dist_to_center']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2022-11-10 20:35:05,949]\u001b[0m Trial 1 failed because of the following error: KeyboardInterrupt('')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_400258/2244277198.py\", line 41, in <lambda>\n",
      "    objective_fn = lambda trial: objective(trial, param_grid_fn)\n",
      "  File \"/tmp/ipykernel_400258/2244277198.py\", line 12, in objective\n",
      "    cbr.fit(\n",
      "  File \"/home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/catboost/core.py\", line 5730, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
      "  File \"/home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/catboost/core.py\", line 2355, in _fit\n",
      "    self._train(\n",
      "  File \"/home/koholm/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/catboost/core.py\", line 1759, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 4623, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 4672, in _catboost._CatBoost._train\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [258], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m non_tunable_cb_params, tuned_params \u001b[39m=\u001b[39m get_hyper_parameters(get_cb_params)\n\u001b[1;32m      3\u001b[0m \u001b[39m# Slower, but due to an issue with Catboost, training on the CPU often yields a better result than on the GPU \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# non_tunable_cb_params['task_type'] = 'CPU'\u001b[39;00m\n\u001b[1;32m      6\u001b[0m train_pool, valid_pool \u001b[39m=\u001b[39m get_cb_pools()\n",
      "Cell \u001b[0;32mIn [257], line 42\u001b[0m, in \u001b[0;36mget_hyper_parameters\u001b[0;34m(param_grid_fn, n_trials)\u001b[0m\n\u001b[1;32m     36\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[1;32m     37\u001b[0m     study_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhyperparam-tuning\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     38\u001b[0m     pruner\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39mpruners\u001b[39m.\u001b[39mMedianPruner(n_warmup_steps\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m), \n\u001b[1;32m     39\u001b[0m     direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m objective_fn \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m trial: objective(trial, param_grid_fn)\n\u001b[0;32m---> 42\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective_fn, n_trials\u001b[39m=\u001b[39;49mn_trials, timeout\u001b[39m=\u001b[39;49m\u001b[39m900\u001b[39;49m) \n\u001b[1;32m     44\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNumber of finished trials: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(study\u001b[39m.\u001b[39mtrials)))\n\u001b[1;32m     46\u001b[0m trial \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/study/study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    324\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    325\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     _optimize(\n\u001b[1;32m    420\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    421\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    422\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    423\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    424\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    425\u001b[0m         catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[1;32m    426\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    427\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    428\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    429\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/study/_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    230\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    231\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    233\u001b[0m ):\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn [257], line 41\u001b[0m, in \u001b[0;36mget_hyper_parameters.<locals>.<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_hyper_parameters\u001b[39m(param_grid_fn: Callable, n_trials\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m):\n\u001b[1;32m     36\u001b[0m     study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[1;32m     37\u001b[0m         study_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhyperparam-tuning\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     38\u001b[0m         pruner\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39mpruners\u001b[39m.\u001b[39mMedianPruner(n_warmup_steps\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m), \n\u001b[1;32m     39\u001b[0m         direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     40\u001b[0m     )\n\u001b[0;32m---> 41\u001b[0m     objective_fn \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m trial: objective(trial, param_grid_fn)\n\u001b[1;32m     42\u001b[0m     study\u001b[39m.\u001b[39moptimize(objective_fn, n_trials\u001b[39m=\u001b[39mn_trials, timeout\u001b[39m=\u001b[39m\u001b[39m900\u001b[39m) \n\u001b[1;32m     44\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNumber of finished trials: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(study\u001b[39m.\u001b[39mtrials)))\n",
      "Cell \u001b[0;32mIn [257], line 12\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, param_grid_fn)\u001b[0m\n\u001b[1;32m     10\u001b[0m     cbr \u001b[39m=\u001b[39m cb\u001b[39m.\u001b[39mCatBoostRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnon_tunable_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtunable_params) \n\u001b[1;32m     11\u001b[0m     train_pool, valid_pool \u001b[39m=\u001b[39m get_cb_pools()\n\u001b[0;32m---> 12\u001b[0m     cbr\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     13\u001b[0m         train_pool,\n\u001b[1;32m     14\u001b[0m         eval_set\u001b[39m=\u001b[39;49m[(X_val_cb, y_val)],\n\u001b[1;32m     15\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m         early_stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m     )\n\u001b[1;32m     18\u001b[0m     y_pred \u001b[39m=\u001b[39m cbr\u001b[39m.\u001b[39mpredict(X_val_cb)\n\u001b[1;32m     20\u001b[0m \u001b[39melif\u001b[39;00m model_name \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlgb\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/catboost/core.py:5730\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5727\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m   5728\u001b[0m     CatBoostRegressor\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m-> 5730\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline,\n\u001b[1;32m   5731\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[1;32m   5732\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[1;32m   5733\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/catboost/core.py:2355\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2351\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2353\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[1;32m   2354\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[0;32m-> 2355\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[1;32m   2356\u001b[0m         train_pool,\n\u001b[1;32m   2357\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   2358\u001b[0m         params,\n\u001b[1;32m   2359\u001b[0m         allow_clear_pool,\n\u001b[1;32m   2360\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m   2361\u001b[0m     )\n\u001b[1;32m   2363\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[0;32m~/Desktop/maskinprosjekt/venv/lib/python3.8/site-packages/catboost/core.py:1759\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1758\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[0;32m-> 1759\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   1760\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[0;32m_catboost.pyx:4623\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:4672\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "non_tunable_cb_params, tuned_params = get_hyper_parameters(get_cb_params)\n",
    "\n",
    "# Slower, but due to an issue with Catboost, training on the CPU often yields a better result than on the GPU \n",
    "# non_tunable_cb_params['task_type'] = 'CPU'\n",
    "\n",
    "train_pool, valid_pool = get_cb_pools()\n",
    "cbm = cb.CatBoostRegressor(**non_tunable_cb_params, **tuned_params, iterations=1000)\n",
    "# cbm.fit(train_pool, eval_set=valid_pool, verbose=50, plot=True, early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance = model.feature_importances_\n",
    "# sorted_idx = np.argsort(feature_importance)\n",
    "# fig = plt.figure(figsize=(12, 6))\n",
    "# plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
    "# plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])\n",
    "# plt.title('Feature Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_tunable_lgb_params, tunable_lgb_params = get_hyper_parameters(get_lgb_params, n_trials=200)\n",
    "\n",
    "# dtrain_lgb, dvalid_lgb = get_lgb_dmatrices()\n",
    "# gbm = lgb.train(\n",
    "#     params={**non_tunable_lgb_params, **tunable_lgb_params},\n",
    "#     train_set=dtrain_lgb,\n",
    "#     valid_sets=dvalid_lgb,\n",
    "#     verbose_eval=False\n",
    "# )\n",
    "# y_val_pred = np.expm1(gbm.predict(X_val_lgb))\n",
    "# score = rmsle(np.expm1(y_val), y_val_pred)\n",
    "# print(score)\n",
    "\n",
    "lgbm = lgb.LGBMRegressor(**non_tunable_lgb_params, **tunable_lgb_params)\n",
    "\n",
    "# lgbm_model.booster_.save_model(f'models/lgbm/{score:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lvl0 = list()\n",
    "lvl0.extend([\n",
    "    ('cbm', cbm),\n",
    "    ('lgbm', lgbm)\n",
    "])\n",
    "\n",
    "lvl1 = LinearRegression()\n",
    "\n",
    "model = StackingRegressor(estimators=lvl0, final_estimator=lvl1, cv=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = np.expm1(model.predict(X_val))\n",
    "score = rmsle(np.expm1(y_val), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = np.expm1(model.predict(X_test))\n",
    "# submission['predicted'] = y_pred\n",
    "# submission.to_csv('submissions/submission.csv', index=False)\n",
    "# model.save_model(f'models/{model.best_score_[\"validation\"][\"RMSE\"]:.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1beb4c19f9c9850d9088b15a4a1b3063555a573f9fb8d1ae667cbe7a8ada917e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
