{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import warnings\n",
    "import os.path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopy\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import shutil\n",
    "import geopandas as gpd\n",
    "import catboost as cb\n",
    "\n",
    "from xgboost import XGBClassifier, plot_importance, to_graphviz, plot_tree\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "from k_fold import random_k_fold\n",
    "from shapely import wkt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from utils import squared_log, rmsle_xgb, add_city_centre_dist, group_ages, to_categorical, nan_to_string, object_encoder\n",
    "from k_fold import random_k_fold, _rmsle\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "spatial = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "age = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "income = pd.read_csv('data/grunnkrets_income_households.csv')\n",
    "households = pd.read_csv('data/grunnkrets_households_num_persons.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "plaace = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "busstops = pd.read_csv('data/busstops_norway.csv')\n",
    "\n",
    "train = pd.read_csv('data/stores_train.csv')\n",
    "test = pd.read_csv('data/stores_test.csv')\n",
    "\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "model_name = \"modeling/0002.model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df: pd.DataFrame, predictor: str = ''):\n",
    "    features = ['store_id', 'year', 'store_name', 'mall_name', 'chain_name', 'address', 'lat', 'lon',\n",
    "                'plaace_hierarchy_id', 'grunnkrets_id']\n",
    "    _X = df[features]\n",
    "\n",
    "    _X['store_name'] = _X['store_name']\n",
    "    _X['address'] = _X['address']\n",
    "    _X['mall_name'] = _X['mall_name']\n",
    "    _X['chain_name'] = _X['chain_name']\n",
    "    _X['plaace_hierarchy_id'] = _X['plaace_hierarchy_id']\n",
    "    # _X['latlon'] = f'{_X.lat}{_X.lon}'\n",
    "    # _X['latlon'] = _X['latlon'].astype('category')\n",
    "\n",
    "    # Merge spatial data\n",
    "    _X = _X.merge(spatial.drop(columns=['year']).drop_duplicates(subset=['grunnkrets_id']), on='grunnkrets_id', how='left')\n",
    "    _X['grunnkrets_name'] = _X['grunnkrets_name']\n",
    "    _X['district_name'] = _X['district_name']\n",
    "    _X['municipality_name'] = _X['municipality_name']\n",
    "    _X = _X.drop(columns=['geometry'])\n",
    "\n",
    "    # Merge age data\n",
    "    age_ranges = [\n",
    "        (0, 19),\n",
    "        (20, 39),\n",
    "        (40, 59),\n",
    "        (60, 79),\n",
    "        (80, 90),\n",
    "    ]\n",
    "    grouped_ages = group_ages(age, age_ranges)\n",
    "    _X = _X.merge(grouped_ages, on='grunnkrets_id', how='left')\n",
    "\n",
    "    # Merge income data\n",
    "    _X = _X.merge(income.drop(columns=['year']).drop_duplicates(subset='grunnkrets_id'), how='left')\n",
    "\n",
    "    # Merge household data\n",
    "    _X = _X.merge(households.drop(columns=['year']).drop_duplicates(subset='grunnkrets_id'), how='left')\n",
    "\n",
    "    # Merge plaace data\n",
    "    _X = _X.merge(plaace.drop_duplicates(subset='plaace_hierarchy_id'), how='left')\n",
    "    _X['plaace_hierarchy_id'] = _X['plaace_hierarchy_id']\n",
    "    _X['sales_channel_name'] = _X['sales_channel_name']\n",
    "    _X = _X.drop(columns=['lv1', 'lv2', 'lv3', 'lv4'])\n",
    "    _X['lv1_desc'] = _X['lv1_desc']\n",
    "    _X['lv2_desc'] = _X['lv2_desc']\n",
    "    _X['lv3_desc'] = _X['lv3_desc']\n",
    "    _X['lv4_desc'] = _X['lv4_desc']\n",
    "    \n",
    "    _X = add_city_centre_dist(_X).drop(columns=['lon_center', 'lat_center'])\n",
    "\n",
    "    # Merge bus data\n",
    "    bus_data_train = gpd.read_parquet('derived_data/stores_bus_stops_lt_1km_train')\n",
    "    _X = _X.merge(bus_data_train.drop(columns=['geometry']), on='store_id', how='left')\n",
    "\n",
    "    _X = _X.drop(columns=['grunnkrets_id', 'plaace_hierarchy_id', 'year', 'store_id'])\n",
    "\n",
    "    if predictor == 'xgb':\n",
    "        # _X = to_categorical(_X)\n",
    "        _X = object_encoder(_X)\n",
    "    elif predictor == 'catboost':\n",
    "        print('hei')\n",
    "        _X = nan_to_string(_X)\n",
    "        print(_X.isna().sum())\n",
    "        \n",
    "    return _X\n",
    "\n",
    "\n",
    "label_name = 'revenue'\n",
    "X = train.drop(columns=[label_name])\n",
    "y = train[label_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(data):\n",
    "  df = data[['revenue', \n",
    "    # 'age_0_19', 'age_20_39', 'age_40_59', 'age_60_79', 'age_80_90', \n",
    "    # 'bus_stops_count', 'Mangler viktighetsniv√•', 'Standard holdeplass', 'Lokalt knutepunkt', 'Nasjonalt knutepunkt', 'Regionalt knutepunkt', 'Annen viktig holdeplass', \n",
    "    'dist_to_center', 'lat','lon'\n",
    "    ]]\n",
    "  df['knutepunkt'] = data[['Lokalt knutepunkt', 'Nasjonalt knutepunkt', 'Regionalt knutepunkt']].sum(axis=1)\n",
    "  # df.revenue = np.exp(df.revenue)\n",
    "  # df.bus_stops_count = np.sqrt(df.bus_stops_count)\n",
    "  df = df[df.dist_to_center < 70_000]\n",
    "  # df.dist_to_center = np.log(df.dist_to_center)\n",
    "  \n",
    "  plt.figure(figsize=(15, 15))\n",
    "  pairplot = sns.pairplot(df)\n",
    "  # heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\n",
    "\n",
    "\n",
    "# data_full =  pd.merge(X_train, y_train, left_index=True, right_index=True) \n",
    "# plot_corr(data_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_buffers(X_train, y_train, X_val, y_val):\n",
    "    # Clear buffers\n",
    "    folder = os.path.join(os.getcwd(), 'modeling')\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "            print(f'Deleted file: {file_path}')\n",
    "\n",
    "    train_buffer_path = 'modeling/train.buffer'\n",
    "    test_buffer_path = 'modeling/test.buffer'\n",
    "\n",
    "    dtrain = xgb.DMatrix(data=X_train, label=np.log1p(y_train), enable_categorical=True)\n",
    "    dtrain.save_binary(train_buffer_path)\n",
    "    print(f'--> {train_buffer_path} created and saved.')\n",
    "\n",
    "    dvalid = xgb.DMatrix(data=X_val, label=y_val, enable_categorical=True)\n",
    "    dvalid.save_binary(test_buffer_path)\n",
    "    print(f'--> {test_buffer_path} created and saved.')\n",
    "\n",
    "    return dtrain, dvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing and creating buffers...\n",
      "Deleted file: c:\\dev\\maskin\\maskinprosjekt\\modeling\\0002.model\n",
      "Deleted file: c:\\dev\\maskin\\maskinprosjekt\\modeling\\test.buffer\n",
      "Deleted file: c:\\dev\\maskin\\maskinprosjekt\\modeling\\train.buffer\n",
      "--> modeling/train.buffer created and saved.\n",
      "--> modeling/test.buffer created and saved.\n",
      "<xgboost.core.DMatrix object at 0x0000023DBB0D93D0> <xgboost.core.DMatrix object at 0x0000023DBB8E5C40>\n",
      "Attempting to initialize parameters for training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [60], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m X_train, X_val, y_train, y_val \u001b[39m=\u001b[39m train_test_split(X, y, train_size\u001b[39m=\u001b[39m\u001b[39m.8\u001b[39m)\n\u001b[0;32m     58\u001b[0m X_train, X_val \u001b[39m=\u001b[39m generate_features(X_train, predictor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mxgb\u001b[39m\u001b[39m'\u001b[39m), generate_features(X_val, predictor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mxgb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m model \u001b[39m=\u001b[39m train_xgb_model(X_train, y_train, X_val, y_val)\n",
      "Cell \u001b[1;32mIn [60], line 10\u001b[0m, in \u001b[0;36mtrain_xgb_model\u001b[1;34m(X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAttempting to initialize parameters for training...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m params \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mcolsample_bytree\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.7717138210314867\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.047506668950627134\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m8\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmin_child_weight\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m3\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m223\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msubsample\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.9929036803032936\u001b[39m}\n\u001b[1;32m---> 10\u001b[0m rand_src_model \u001b[39m=\u001b[39m random_k_fold(X_train, y_train, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, n_iter\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[0;32m     11\u001b[0m params \u001b[39m=\u001b[39m rand_src_model\u001b[39m.\u001b[39mbest_params_\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m(params)\n",
      "File \u001b[1;32mc:\\dev\\maskin\\maskinprosjekt\\k_fold.py:50\u001b[0m, in \u001b[0;36mrandom_k_fold\u001b[1;34m(X, y, model, params, k, n_iter, verbose, n_jobs)\u001b[0m\n\u001b[0;32m     45\u001b[0m randm_src \u001b[39m=\u001b[39m RandomizedSearchCV(model, param_distributions\u001b[39m=\u001b[39mparams, n_iter\u001b[39m=\u001b[39mn_iter,\n\u001b[0;32m     46\u001b[0m                                scoring\u001b[39m=\u001b[39m_rmsle_scorer, verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m     47\u001b[0m                                cv\u001b[39m=\u001b[39mkfold\u001b[39m.\u001b[39msplit(X, y), n_jobs\u001b[39m=\u001b[39mn_jobs)\n\u001b[0;32m     49\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 50\u001b[0m model \u001b[39m=\u001b[39m randm_src\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[0;32m     51\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRandomizedSearchCV took \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m seconds for \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m candidates\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m parameter settings.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m ((time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start), n_iter))\n\u001b[0;32m     54\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\dev\\maskin\\maskinprosjekt\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\dev\\maskin\\maskinprosjekt\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1753\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1751\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1752\u001b[0m     \u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1753\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1754\u001b[0m         ParameterSampler(\n\u001b[0;32m   1755\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1756\u001b[0m         )\n\u001b[0;32m   1757\u001b[0m     )\n",
      "File \u001b[1;32mc:\\dev\\maskin\\maskinprosjekt\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    819\u001b[0m         )\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    824\u001b[0m         clone(base_estimator),\n\u001b[0;32m    825\u001b[0m         X,\n\u001b[0;32m    826\u001b[0m         y,\n\u001b[0;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    833\u001b[0m     )\n\u001b[0;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    836\u001b[0m     )\n\u001b[0;32m    837\u001b[0m )\n\u001b[0;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     )\n",
      "File \u001b[1;32mc:\\dev\\maskin\\maskinprosjekt\\venv\\lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\dev\\maskin\\maskinprosjekt\\venv\\lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\dev\\maskin\\maskinprosjekt\\venv\\lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py:440\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m    438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[1;32m--> 440\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    443\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_xgb_model(X_train, y_train, X_val, y_val):\n",
    "    print('Clearing and creating buffers...')\n",
    "    dtrain, dvalid = clear_buffers(X_train, y_train, X_val, y_val)\n",
    "    print(dtrain, dvalid)\n",
    "\n",
    "    print(\"Attempting to initialize parameters for training...\")\n",
    "\n",
    "    params = {'colsample_bytree': 0.7717138210314867, 'learning_rate': 0.047506668950627134, 'max_depth': 8, 'min_child_weight': 3, 'n_estimators': 223, 'subsample': 0.9929036803032936}\n",
    "    \n",
    "    rand_src_model = random_k_fold(X_train, y_train, verbose=0, n_iter=20)\n",
    "    params = rand_src_model.best_params_\n",
    "    print(params)\n",
    "\n",
    "    # params = {'colsample_bytree': 0.8601277878899238, 'eval_metric': _rmsle, 'gamma': 0.12760202929262826, 'learning_rate': 0.07356461924449906, 'max_depth': 5, 'min_child_weight': 1, 'n_estimators': 306, 'objective': 'reg:squaredlogerror', 'subsample': 0.8993341396761092}\n",
    "\n",
    "    params['disable_default_eval_metric'] = True\n",
    "    print(\"--> parameters for training initialized.\")\n",
    "\n",
    "    # y_pred = model.predict(dvalid)\n",
    "    # print(_rmsle(y_val, y_pred))\n",
    "\n",
    "    # X_test = generate_features(test, predictor='xgb')\n",
    "    # dtest = xgb.DMatrix(data=X_test, enable_categorical=True)\n",
    "\n",
    "    # print(\"\\nAttempting to start prediction...\")\n",
    "    # y_pred = model.predict(dtest, ntree_limit=model.best_iteration)\n",
    "    # print(\"--> Prediction finished.\")\n",
    "\n",
    "    # print(\"\\nAttempting to save prediction...\")\n",
    "    # submission['predicted'] = np.array(y_pred)\n",
    "    # submission.to_csv('submissions/submission.csv', index=False)\n",
    "    # print(\"--> prediction saved with features as name in submission folder.\")\n",
    "\n",
    "    num_round = 999\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "    print(\"Attempting to start training...\")\n",
    "    model = xgb.train(\n",
    "        params=params, \n",
    "        dtrain=dtrain, \n",
    "        num_boost_round=num_round, \n",
    "        obj=squared_log,\n",
    "        custom_metric=rmsle_xgb,\n",
    "        evals=watchlist, \n",
    "        early_stopping_rounds=10, \n",
    "        verbose_eval=20)\n",
    "    print(\"--> model trained.\")\n",
    "    print('Best score:', model.best_score)\n",
    "\n",
    "    print(\"Attempting to save model...\")\n",
    "    model.save_model(model_name)\n",
    "    print(\"--> model saved.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=.8)\n",
    "X_train, X_val = generate_features(X_train, predictor='xgb'), generate_features(X_val, predictor='xgb')\n",
    "\n",
    "model = train_xgb_model(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to start prediction...\n",
      "--> Prediction finished.\n",
      "\n",
      "Attempting to save prediction...\n",
      "--> prediction saved with features as name in submission folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\maskin\\maskinprosjekt\\venv\\lib\\site-packages\\xgboost\\core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def xgb_prediction(X_test, model):\n",
    "    dtest = xgb.DMatrix(data=X_test, enable_categorical=True)\n",
    "\n",
    "    print(\"\\nAttempting to start prediction...\")\n",
    "    y_pred = model.predict(dtest, ntree_limit=model.best_iteration)\n",
    "    print(\"--> Prediction finished.\")\n",
    "\n",
    "    print(\"\\nAttempting to save prediction...\")\n",
    "    submission['predicted'] = np.array(y_pred)\n",
    "    submission.to_csv('submissions/submission.csv', index=False)\n",
    "    print(\"--> prediction saved with features as name in submission folder.\")\n",
    "\n",
    "\n",
    "X_test = generate_features(test, predictor='xgb')\n",
    "xgb_prediction(X_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(model)\n",
    "xgb.to_graphviz(model, num_trees=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare features for Catboost predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=.8)\n",
    "# X_train, X_val = generate_features(X_train, predictor='catboost'), generate_features(X_val, predictor='catboost')\n",
    "# X_test = generate_features(test, predictor='catboost')\n",
    "\n",
    "# cat_features = list(X_train.select_dtypes(include=[object]).columns)\n",
    "\n",
    "# train_pool = cb.Pool(X_train, y_train, cat_features=cat_features)\n",
    "# test_pool = cb.Pool(X_test, cat_features=cat_features)\n",
    "\n",
    "# # X_train[X_train.columns[X_train.isna().any()].tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'depth': randint(2, 20),\n",
    "#     'learning_rate': uniform(0.01, 0.4),\n",
    "#     'iterations': randint(10, 1000)\n",
    "# }\n",
    "\n",
    "# model = cb.CatBoostRegressor(loss_function='RMSE')\n",
    "\n",
    "# model.randomized_search(train_pool, param_distributions=params, cv=5)\n",
    "\n",
    "# pred = model.predict(X_val)\n",
    "# rmse = (np.sqrt(mean_squared_log_error(y_val, pred)))\n",
    "# print('Testing performance')\n",
    "# print('RMSE: {:.2f}'.format(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1beb4c19f9c9850d9088b15a4a1b3063555a573f9fb8d1ae667cbe7a8ada917e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
