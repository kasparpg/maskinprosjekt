{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import warnings\n",
    "import os.path\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopy\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from shapely import wkt\n",
    "from retail_revenue_xgb import generate_features, create_buffer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import squared_log, rmsle\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial = pd.read_csv('data/grunnkrets_norway_stripped.csv')\n",
    "age = pd.read_csv('data/grunnkrets_age_distribution.csv')\n",
    "income = pd.read_csv('data/grunnkrets_income_households.csv')\n",
    "households = pd.read_csv('data/grunnkrets_households_num_persons.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "plaace = pd.read_csv('data/plaace_hierarchy.csv')\n",
    "busstops = pd.read_csv('data/busstops_norway.csv')\n",
    "\n",
    "\n",
    "def generate_features(df: pd.DataFrame):\n",
    "    features = ['store_id', 'year', 'store_name', 'mall_name', 'chain_name', 'address', 'lat', 'lon', 'grunnkrets_id',\n",
    "                'plaace_hierarchy_id']\n",
    "    df = df[features]\n",
    "    df['store_name'] = df['store_name'].astype('category')\n",
    "    df['store_id'] = df['store_id'].astype('category')\n",
    "    df['address'] = df['address'].astype('category')\n",
    "    df['chain_name+mall_name'] = (df['chain_name'] + df['mall_name']).astype('category')\n",
    "    df['mall_name'] = df['mall_name'].astype('category')\n",
    "    df['chain_name'] = df['chain_name'].astype('category')\n",
    "    df['plaace_hierarchy_id'] = df['plaace_hierarchy_id'].astype('category')\n",
    "\n",
    "    # attempt to difference the lat and lon values, as they seem to be somewhat trending negatively.\n",
    "    df['lon'] = df['lon'].diff()\n",
    "    df['lat'] = df['lat'].diff()\n",
    "\n",
    "    # remove duplicates and merge with the spatial data.\n",
    "    spatial.drop_duplicates(subset=['grunnkrets_id'])\n",
    "    df = pd.merge(df, spatial.drop_duplicates(subset=['grunnkrets_id']), how='left')\n",
    "    df['grunnkrets_name'] = df['grunnkrets_name'].astype('category')\n",
    "    df['district_name'] = df['district_name'].astype('category')\n",
    "    df['municipality_name'] = df['municipality_name'].astype('category')\n",
    "    df['geometry'] = df['geometry'].astype('category')\n",
    "\n",
    "    age.drop_duplicates(subset=['grunnkrets_id'])\n",
    "    df = pd.merge(df, age.drop_duplicates(subset=['grunnkrets_id']), how='left')\n",
    "\n",
    "    income.drop_duplicates(subset=['grunnkrets_id'])\n",
    "    df = pd.merge(df, income.drop_duplicates(subset=['grunnkrets_id']), how='left')\n",
    "\n",
    "    households.drop_duplicates(subset=['grunnkrets_id'])\n",
    "    df = pd.merge(df, households.drop_duplicates(subset=['grunnkrets_id']), how='left')\n",
    "\n",
    "    plaace.drop_duplicates(subset=['plaace_hierarchy_id'])\n",
    "    df = pd.merge(df, plaace.drop_duplicates(subset=['plaace_hierarchy_id']), how='left')\n",
    "    df['plaace_hierarchy_id'] = df['plaace_hierarchy_id'].astype('category')\n",
    "    df['sales_channel_name'] = df['sales_channel_name'].astype('category')\n",
    "    df['lv1_desc'] = df['lv1_desc'].astype('category')\n",
    "    df['lv2_desc'] = df['lv2_desc'].astype('category')\n",
    "    df['lv3'] = df['lv3'].astype('category')\n",
    "    df['lv3_desc'] = df['lv3_desc'].astype('category')\n",
    "    df['lv4'] = df['lv4'].astype('category')\n",
    "    df['lv4_desc'] = df['lv4_desc'].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = pd.read_csv('data/stores_train.csv')\n",
    "test = pd.read_csv('data/stores_test.csv')\n",
    "\n",
    "label_name = 'revenue'\n",
    "X = train.drop(columns=[label_name])\n",
    "y = train[label_name]\n",
    "\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "model_to_load = \"modeling/0002.model\"\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=.8)\n",
    "\n",
    "X_train, X_val = generate_features(X_train), generate_features(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted file: c:\\dev\\maskin\\maskinprosjekt\\modeling\\0002.model\n",
      "Deleted file: c:\\dev\\maskin\\maskinprosjekt\\modeling\\test.buffer\n",
      "Deleted file: c:\\dev\\maskin\\maskinprosjekt\\modeling\\train.buffer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oskar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\data.py:322: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  transformed[data.columns[i]] = data[data.columns[i]]\n",
      "c:\\Users\\oskar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\data.py:313: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  transformed[data.columns[i]] = (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> modeling/train.buffer created and saved.\n",
      "--> modeling/test.buffer created and saved.\n"
     ]
    }
   ],
   "source": [
    "# Clear buffers\n",
    "folder = os.path.join(os.getcwd(), 'modeling')\n",
    "for filename in os.listdir(folder):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.unlink(file_path)\n",
    "        print(f'Deleted file: {file_path}')\n",
    "\n",
    "train_buffer_path = 'modeling/train.buffer'\n",
    "test_buffer_path = 'modeling/test.buffer'\n",
    "\n",
    "dtrain = xgb.DMatrix(data=X_train, label=y_train, enable_categorical=True)\n",
    "dtrain.save_binary(train_buffer_path)\n",
    "print(f'--> {train_buffer_path} created and saved.')\n",
    "\n",
    "dvalid = xgb.DMatrix(data=X_val, label=y_val, enable_categorical=True)\n",
    "dvalid.save_binary(test_buffer_path)\n",
    "print(f'--> {test_buffer_path} created and saved.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No model found. Attempt at creating a new one will now start:\n",
      "Attempting to initialize parameters for training...\n",
      "--> parameters for training initialized.\n",
      "Attempting to start training...\n",
      "[0]\ttrain-PyRMSLE:1.52088\tvalid-PyRMSLE:1.53014\n",
      "[50]\ttrain-PyRMSLE:0.64049\tvalid-PyRMSLE:0.86840\n",
      "[79]\ttrain-PyRMSLE:0.59995\tvalid-PyRMSLE:0.86187\n",
      "--> model trained.\n",
      "Attempting to save model...\n",
      "--> model saved.\n",
      "\n",
      "Attempting to start prediction...\n",
      "--> Prediction finished.\n",
      "\n",
      "Attempting to save prediction...\n",
      "--> prediction saved with features as name in submission folder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oskar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\xgboost\\core.py:91: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# check if there already exists a model.\n",
    "if os.path.exists(model_to_load):\n",
    "    print(\"\\nModel found, attempting to load.\")\n",
    "    bst = xgb.Booster({'nthread': 4, 'disable_default_eval_metric': True})  # init model\n",
    "    bst.load_model(model_to_load)  # load data\n",
    "    print(\"--> model successfully loaded.\")\n",
    "else:\n",
    "    print(\"\\nNo model found. Attempt at creating a new one will now start:\")\n",
    "    print(\"Attempting to initialize parameters for training...\")\n",
    "    params = {'max_depth': 10, 'eta': 0.1, 'disable_default_eval_metric': True}\n",
    "    num_round = 1000\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "    print(\"--> parameters for training initialized.\")\n",
    "\n",
    "    print(\"Attempting to start training...\")\n",
    "    bst = xgb.train(\n",
    "        params=params, \n",
    "        dtrain=dtrain, \n",
    "        num_boost_round=num_round, \n",
    "        obj=squared_log,\n",
    "        custom_metric=rmsle,\n",
    "        evals=watchlist, \n",
    "        early_stopping_rounds=10, \n",
    "        verbose_eval=50)\n",
    "    print(\"--> model trained.\")\n",
    "\n",
    "    print(\"Attempting to save model...\")\n",
    "    bst.save_model(model_to_load)\n",
    "    print(\"--> model saved.\")\n",
    "\n",
    "X_test = generate_features(test)\n",
    "dtest = xgb.DMatrix(data=X_test, enable_categorical=True)\n",
    "\n",
    "print(\"\\nAttempting to start prediction...\")\n",
    "y_pred = bst.predict(dtest, ntree_limit=bst.best_iteration)\n",
    "print(\"--> Prediction finished.\")\n",
    "\n",
    "print(\"\\nAttempting to save prediction...\")\n",
    "submission['predicted'] = np.array(y_pred)\n",
    "submission.to_csv('submissions/submission.csv', index=False)\n",
    "print(\"--> prediction saved with features as name in submission folder.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "882a6b7fc3ca80e688227decd54209862062b8721a918d514e0ed60576137ba8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
